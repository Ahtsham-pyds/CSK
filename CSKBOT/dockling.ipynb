{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1751518161.267915 3806843 fork_posix.cc:77] Other threads are currently calling into gRPC, skipping fork() handlers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-index in /opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages (0.12.44)\n",
      "Requirement already satisfied: openai in /opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages (1.91.0)\n",
      "Requirement already satisfied: llama-index-agent-openai<0.5,>=0.4.0 in /opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages (from llama-index) (0.4.11)\n",
      "Requirement already satisfied: llama-index-cli<0.5,>=0.4.2 in /opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages (from llama-index) (0.4.3)\n",
      "Requirement already satisfied: llama-index-core<0.13,>=0.12.44 in /opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages (from llama-index) (0.12.44)\n",
      "Requirement already satisfied: llama-index-embeddings-openai<0.4,>=0.3.0 in /opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages (from llama-index) (0.3.1)\n",
      "Requirement already satisfied: llama-index-indices-managed-llama-cloud>=0.4.0 in /opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages (from llama-index) (0.7.7)\n",
      "Requirement already satisfied: llama-index-llms-openai<0.5,>=0.4.0 in /opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages (from llama-index) (0.4.7)\n",
      "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.6,>=0.5.0 in /opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages (from llama-index) (0.5.1)\n",
      "Requirement already satisfied: llama-index-program-openai<0.4,>=0.3.0 in /opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages (from llama-index) (0.3.2)\n",
      "Requirement already satisfied: llama-index-question-gen-openai<0.4,>=0.3.0 in /opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages (from llama-index) (0.3.1)\n",
      "Requirement already satisfied: llama-index-readers-file<0.5,>=0.4.0 in /opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages (from llama-index) (0.4.9)\n",
      "Requirement already satisfied: llama-index-readers-llama-parse>=0.4.0 in /opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages (from llama-index) (0.4.0)\n",
      "Requirement already satisfied: nltk>3.8.1 in /opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages (from llama-index) (3.9.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages (from openai) (4.8.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages (from openai) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages (from openai) (0.8.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages (from openai) (2.11.5)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages (from openai) (4.13.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: aiohttp<4,>=3.8.6 in /opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages (from llama-index-core<0.13,>=0.12.44->llama-index) (3.11.10)\n",
      "Requirement already satisfied: aiosqlite in /opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages (from llama-index-core<0.13,>=0.12.44->llama-index) (0.21.0)\n",
      "Requirement already satisfied: banks<3,>=2.0.0 in /opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages (from llama-index-core<0.13,>=0.12.44->llama-index) (2.1.2)\n",
      "Requirement already satisfied: dataclasses-json in /opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages (from llama-index-core<0.13,>=0.12.44->llama-index) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages (from llama-index-core<0.13,>=0.12.44->llama-index) (1.2.18)\n",
      "Requirement already satisfied: dirtyjson<2,>=1.0.8 in /opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages (from llama-index-core<0.13,>=0.12.44->llama-index) (1.0.8)\n",
      "Requirement already satisfied: filetype<2,>=1.2.0 in /opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages (from llama-index-core<0.13,>=0.12.44->llama-index) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages (from llama-index-core<0.13,>=0.12.44->llama-index) (2024.3.1)\n",
      "Requirement already satisfied: llama-index-workflows<2,>=1.0.1 in /opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages (from llama-index-core<0.13,>=0.12.44->llama-index) (1.0.1)\n",
      "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in /opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages (from llama-index-core<0.13,>=0.12.44->llama-index) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages (from llama-index-core<0.13,>=0.12.44->llama-index) (3.4.2)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages (from llama-index-core<0.13,>=0.12.44->llama-index) (2.2.6)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages (from llama-index-core<0.13,>=0.12.44->llama-index) (11.1.0)\n",
      "Requirement already satisfied: pyyaml>=6.0.1 in /opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages (from llama-index-core<0.13,>=0.12.44->llama-index) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.31.0 in /opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages (from llama-index-core<0.13,>=0.12.44->llama-index) (2.32.3)\n",
      "Requirement already satisfied: setuptools>=80.9.0 in /opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages (from llama-index-core<0.13,>=0.12.44->llama-index) (80.9.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.49 in /opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.13,>=0.12.44->llama-index) (2.0.36)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages (from llama-index-core<0.13,>=0.12.44->llama-index) (8.5.0)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in /opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages (from llama-index-core<0.13,>=0.12.44->llama-index) (0.8.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages (from llama-index-core<0.13,>=0.12.44->llama-index) (0.9.0)\n",
      "Requirement already satisfied: wrapt in /opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages (from llama-index-core<0.13,>=0.12.44->llama-index) (1.17.1)\n",
      "Requirement already satisfied: llama-cloud==0.1.26 in /opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index) (0.1.26)\n",
      "Requirement already satisfied: beautifulsoup4<5,>=4.12.3 in /opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages (from llama-index-readers-file<0.5,>=0.4.0->llama-index) (4.12.3)\n",
      "Requirement already satisfied: pandas<2.3.0 in /opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages (from llama-index-readers-file<0.5,>=0.4.0->llama-index) (2.2.3)\n",
      "Requirement already satisfied: pypdf<6,>=5.1.0 in /opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages (from llama-index-readers-file<0.5,>=0.4.0->llama-index) (5.1.0)\n",
      "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages (from llama-index-readers-file<0.5,>=0.4.0->llama-index) (0.0.26)\n",
      "Requirement already satisfied: llama-parse>=0.5.0 in /opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages (from llama-index-readers-llama-parse>=0.4.0->llama-index) (0.6.34)\n",
      "Requirement already satisfied: click in /opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages (from nltk>3.8.1->llama-index) (8.1.8)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages (from nltk>3.8.1->llama-index) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages (from nltk>3.8.1->llama-index) (2024.11.6)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.44->llama-index) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.44->llama-index) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.44->llama-index) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.44->llama-index) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.44->llama-index) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.44->llama-index) (0.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.44->llama-index) (1.18.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.44->llama-index) (4.0.3)\n",
      "Requirement already satisfied: griffe in /opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.44->llama-index) (1.7.3)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.44->llama-index) (3.1.5)\n",
      "Requirement already satisfied: platformdirs in /opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.44->llama-index) (4.3.8)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages (from beautifulsoup4<5,>=4.12.3->llama-index-readers-file<0.5,>=0.4.0->llama-index) (2.6)\n",
      "Requirement already satisfied: llama-index-instrumentation>=0.1.0 in /opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages (from llama-index-workflows<2,>=1.0.1->llama-index-core<0.13,>=0.12.44->llama-index) (0.2.0)\n",
      "Requirement already satisfied: llama-cloud-services>=0.6.32 in /opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (0.6.34)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages (from pandas<2.3.0->llama-index-readers-file<0.5,>=0.4.0->llama-index) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages (from pandas<2.3.0->llama-index-readers-file<0.5,>=0.4.0->llama-index) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages (from pandas<2.3.0->llama-index-readers-file<0.5,>=0.4.0->llama-index) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages (from requests>=2.31.0->llama-index-core<0.13,>=0.12.44->llama-index) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages (from requests>=2.31.0->llama-index-core<0.13,>=0.12.44->llama-index) (2.4.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.13,>=0.12.44->llama-index) (3.2.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.13,>=0.12.44->llama-index) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages (from dataclasses-json->llama-index-core<0.13,>=0.12.44->llama-index) (3.24.2)\n",
      "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.1 in /opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages (from llama-cloud-services>=0.6.32->llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (1.0.1)\n",
      "Requirement already satisfied: packaging>=17.0 in /opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13,>=0.12.44->llama-index) (23.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas<2.3.0->llama-index-readers-file<0.5,>=0.4.0->llama-index) (1.16.0)\n",
      "Requirement already satisfied: colorama>=0.4 in /opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages (from griffe->banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.44->llama-index) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages (from jinja2->banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.44->llama-index) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install llama-index openai\n",
    "!pip install -q --progress-bar off llama-index-readers-docling \\\n",
    "llama-index-node-parser-docling llama-index-embeddings-huggingface \\\n",
    "llama-index-llms-huggingface-api llama-index-vector-stores-milvus \\\n",
    "llama-index-readers-file\n",
    "!pip install  --upgrade openai\n",
    "1+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1751622002.719100 6369203 fork_posix.cc:77] Other threads are currently calling into gRPC, skipping fork() handlers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement llama_index.core.chat_engine (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for llama_index.core.chat_engine\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install  llama_index.core.chat_engine \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from tempfile import mkdtemp\n",
    "from warnings import filterwarnings\n",
    "from dotenv import load_dotenv\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core import StorageContext, VectorStoreIndex\n",
    "from llama_index.core.node_parser import MarkdownNodeParser\n",
    "from llama_index.readers.docling import DoclingReader\n",
    "from llama_index.vector_stores.milvus import MilvusVectorStore\n",
    "from llama_index.core.chat_engine import CondenseQuestionChatEngine\n",
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _get_env_from_colab_or_os(key):\n",
    "    try:\n",
    "        from google.colab import userdata\n",
    "\n",
    "        try:\n",
    "            return userdata.get(key)\n",
    "        except userdata.SecretNotFoundError:\n",
    "            pass\n",
    "    except ImportError:\n",
    "        pass\n",
    "    return os.getenv(key)\n",
    "\n",
    "\n",
    "GEN_MODEL = HuggingFaceInferenceAPI(\n",
    "    token=os.getenv(\"HF_TOKEN\"),\n",
    "    model_name=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n"
     ]
    }
   ],
   "source": [
    "#\"\"\n",
    "'''BAAI/bge-small-en-v1.5 - Beijing Academy of Artificial Intelligence (BAAI)\n",
    "Sentence Embedding Model / Text Embedding Model\n",
    "Specifically designed for English text.\n",
    "Based on E5 architecture, which itself is a modification of the MiniLM \n",
    "(or similar lightweight Transformer) architecture optimized for embedding tasks.\n",
    "~60 million parameters'''\n",
    "\n",
    "EMBED_MODEL = HuggingFaceEmbedding(\"BAAI/bge-small-en-v1.5\")\n",
    "embed_dim = len(EMBED_MODEL.get_text_embedding(\"Burger\"))#\n",
    "print(embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE = 'Internal.pdf'\n",
    "#SOURCE = \"https://arxiv.org/pdf/2408.09869\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages/milvus_lite/__init__.py:15: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import DistributionNotFound, get_distribution\n",
      "2025-07-04 10:32:15,535 [DEBUG][_create_connection]: Created new connection using: 16eb007264024252a77f03cd7460e4b7 (async_milvus_client.py:599)\n"
     ]
    }
   ],
   "source": [
    "reader = DoclingReader()\n",
    "node_parser = MarkdownNodeParser()\n",
    "MILVUS_URI = str(Path(mkdtemp())/ 'docling_ahtsham.db')\n",
    "vector_store = MilvusVectorStore(uri=MILVUS_URI,dim=embed_dim,overwrite=True)\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents=reader.load_data(SOURCE),\n",
    "    transformations=[node_parser],\n",
    "    storage_context=StorageContext.from_defaults(vector_store=vector_store),\n",
    "    embed_model=EMBED_MODEL,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['gpt-3.5-turbo','text-davinci-003']\n",
    "\n",
    "llm = OpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0,\n",
    "    max_tokens=512\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What tools and metrics would you use to monitor server hardware performance?\n",
      "A: Use Nagios, Zabbix, Prometheus, or cloud tools like AWS CloudWatch or Azure Monitor to monitor server hardware performance. Metrics to monitor include CPU usage, RAM usage, disk I/O (throughput/latency), disk space, network throughput, and temperatures. SMART data can be used to monitor disk health indicators such as reallocated sectors and error rates. SNMP can be enabled on devices to collect performance data, and logs like syslog or Event Viewer can be centralized to catch hardware errors like fan failures or power issues. Configure dashboards for real-time viewing and set alerts for threshold breaches, such as CPU exceeding 90% or temperature exceeding 75°C. Capacity planning involves analyzing trends to anticipate upgrades, like disk space nearing capacity.\n",
      "\n",
      "Sources:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(\"## Hardware Troubleshooting\\n\\n- Question: How do you diagnose a failing hard disk in a server? 1.\\n- Answer: Disk failures often cause slowdowns or errors. Troubleshooting steps:\\n- SMART Data: Use smartctl or a RAID controller utility to check SMART status (reallocated sectors, pending errors). 2.\\n- Noise Check: Listen for unusual sounds (clicking, beeping). 3.\\n- Log Review: Check system logs (Windows Event Viewer or /var/log/ ) for disk I/O errors. 4.\\n- Disk Check: Run file system checks (CHKDSK on Windows, fsck on Linux) to find bad sectors. 5.\\n- Backup Immediately: Copy critical data to backup storage at first sign of failure. 6.\\n\\n- Replace Disk: If   errors   persist,   replace the drive and rebuild/restore from backups or RAID parity. 7.\\n- Question: Describe the differences between RAID 0, 1, 5, and 10 in terms of redundancy and performance. 8.\\n\\nAnswer: RAID levels offer different trade-offs:\\n\\n- RAID 0 (Striping): No redundancy. Data is striped across disks for high performance, but if any disk fails, all data is lost. 9.\\n- RAID 1 (Mirroring): 1:1 data copy on two disks. High read performance (reads from either disk), safe storage (one disk can fail). 10.\\n- RAID 5 (Striping with Parity): Data is striped plus parity across 3+ disks. Can survive one disk failure. Good read speeds, slower writes (parity overhead). 11.\\n- RAID 10 (Mirrored Stripes): Combines RAID 1+0, requiring 4+ disks. Provides high performance and redundancy (can survive multiple failures if not in the same mirror). 12.\\n- Question: What are best practices for cooling and power management in a data center? Answer: Key best practices: 1.\\n- Hot/Cold Aisle: Arrange racks in hot and cold aisles; use containment to keep cold air in server intakes. 2.\\n- Clear Airflow: Avoid blocking vents; use blanking panels in empty rack spaces. 3.\\n- Temperature Monitoring: Install sensors to keep temperatures in recommended ranges (1827°C, 40-60% humidity). 4.\\n- Redundant Power: Use redundant power supplies and dual power feeds for critical equipment. 5.\\n- UPS &amp; Generators: Provide uninterruptible power supplies for clean shutdowns and generators for long outages. 6.\\n- Efficient Equipment: Choose high-efficiency power supplies (80 PLUS) and servers with power management. 7.\\n- Maintenance: Regularly clean air filters and ensure cooling units function properly. 8.\\n- Policies: Document and enforce environmental policies (e.g., max rack density, humidity levels). 9.\\n- Question: How do you safely replace a failed network switch in a live environment? Answer: Steps for minimal disruption: 10.\\n- Prepare Replacement: Pre-configure the new switch (IP, VLANs, ports) offline if possible. 11.\\n- Notify Stakeholders: Inform users of planned downtime; schedule during maintenance hours. 12.\\n- Backup Config: Save the old switch's configuration (if available) for reference. 13.\\n- Check Redundancy: Confirm any spanning tree or LACP settings won't cause a loop when the switch is removed. 14.\\n\\n|   RAID Level |   Min Disks | Fault Tolerance   | Performance               | Use Case                  |\\n|--------------|-------------|-------------------|---------------------------|---------------------------|\\n|            0 |           2 | None              | Very high                 | Temp data, caches         |\\n|            1 |           2 | 1 disk            | High reads, fair writes   | OS drives, critical data  |\\n|            5 |           3 | 1 disk            | Good reads, slower writes | File servers, backups     |\\n|           10 |           4 | 1 disk per mirror | Excellent                 | Databases, virtualization |\\n\\n- Swap Hardware: Disconnect the failed switch and immediately connect the new one with the same cables. 15.\\n- Power Up: Turn on the new switch and ensure power LEDs are normal. 16.\\n- Verify Connectivity: Test uplinks (ping core/router) and test devices on access ports. 17.\\n- Monitor: Watch for any abnormal behavior in the network; keep the old switch on standby until all is clear . 18.\\n- Question: What considerations are there when installing and configuring a new server rack? Answer: Important considerations: 19.\\n- Space &amp; Layout: Ensure adequate rack units (U) and plan equipment placement (heaviest at bottom). 20.\\n- Power Distribution: Install PDUs and ensure power circuits match equipment needs (voltage, phases). 21.\\n- Cooling: Check that air conditioning can handle the heat load; follow hot/cold aisle design. 22.\\n- Cable Management: Use proper cable trays, ties, and labels for network and power cables. 23.\\n- Grounding: Properly ground the rack and electrical systems to prevent static and surges. 24.\\n- Security: Lock the rack and control physical access. 25.\\n- Weight Rating: Verify the rack's load rating is not exceeded. 26.\\n- Documentation: Record rack diagram, device positions, and port mappings for inventory. 27.\\n- Question: What factors do you consider when selecting an enterprise-grade SSD for a server? Answer: Factors include: 28.\\n- Endurance: Look at Total Bytes Written (TBW); enterprise SSDs have higher endurance to handle heavy writes. 29.\\n- Performance: Consider IOPS and throughput (sequential vs random) based on workload (database, logging, etc.). 30.\\n- Interface: Choose appropriate interface (SATA, SAS, or NVMe PCIe). NVMe drives offer much higher performance if supported. 31.\\n- Capacity: Ensure sufficient capacity; also plan for RAID (mirroring or parity affects usable capacity). 32.\\n- Power-Loss Protection: Enterprise SSDs often include capacitors to preserve data in case of sudden power loss. 33.\\n- Security Features: Hardware encryption (SED) or secure erase features may be required. 34.\\n- Vendor Support &amp; Warranty: Check warranty period (often 3-5 years) and enterprise-grade support options. 35.\\n- Compatibility: Verify the server's hardware compatibility list and ensure firmware updates are available. 36.\\n- Question: How do you manage firmware and BIOS updates on network devices like routers and switches? 37.\\n- Answer: Managing updates:\\n- Inventory: Keep track of current firmware versions for each device. 38.\\n- Vendor Docs: Check release notes and compatibility charts to choose the correct firmware. 39.\\n- Backup Configs: Always back up device configurations before updating firmware. 40.\\n- Test: If possible, test updates on spare or lab devices first. 41.\\n- Schedule: Apply updates during maintenance windows to avoid service disruptions. 42.\\n\\n- Upgrade Process: Follow vendor procedures carefully (copy image file, verify checksum, reboot device). 43.\\n- Verify: After update, confirm firmware version and that settings were retained. Test functionality (e.g., ping neighbors, check routing). 44.\\n- Rollback Plan: Keep older firmware images available in case you need to revert. 45.\\n- Question: When a desktop computer fails to boot, which hardware components do you check first? 46.\\n- Answer: Check in order:\\n- Power Supply: Verify power cable and switch, listen for fan or drive spin. 47.\\n- POST Beeps/Codes: Listen to BIOS beep codes or check any error LEDs to identify faulty component. 48.\\n- Memory (RAM): Reseat RAM sticks; try booting with one module at a time. 49.\\n- Graphics/Display: Reseat GPU or try onboard video; check monitor power and cables. 50.\\n- Connections: Ensure motherboard power connectors (24-pin, 8-pin CPU) are secure. 51.\\n- CPU and Cooling: Confirm CPU cooler is attached properly; check for overheating (unlikely at power-on). 52.\\n- Peripherals: Disconnect USB drives, printers, etc., that could be interfering. 53.\\n- Swap Components: If possible, swap suspected bad parts (PSU, RAM, GPU) with known-good ones to isolate. 54.\\n- 55.\\n- Question: Explain the purpose of ECC RAM in servers.\\n- Answer: ECC   (Error-Correcting   Code)   RAM   detects   and   corrects   single-bit   memory   errors automatically. Its purpose:\\n- Data Integrity: Prevents data corruption by fixing bit flips (caused by electrical interference). 56.\\n- System Stability: Reduces crashes and blue screens due to memory errors. 57.\\n- Reliability: Crucial for servers running critical applications (databases, virtualization) where errors can have big impacts. 58.\\n- Cost vs Benefit: ECC modules cost a bit more and slightly slower due to parity calculations, but they are justified in enterprise for uptime and reliability. 59.\\n- Question: What tools and metrics would you use to monitor server hardware performance? Answer: Tools and metrics: 60.\\n- Monitoring Software: Use Nagios, Zabbix, Prometheus, or cloud tools (AWS CloudWatch, Azure Monitor). ◦\\n- Metrics: CPU usage, RAM usage, disk I/O (throughput/latency), disk space, network throughput, and temperatures. ◦\\n- SMART Data: Monitor disk health indicators (reallocated sectors, error rates) via SMART. ◦\\n- SNMP: Enable SNMP on devices to collect performance data. ◦\\n- Logs: Centralize logs (syslog, Event Viewer) to catch hardware errors (fan failures, power issues). ◦\\n- Dashboards/Alerts: Configure dashboards for real-time viewing and set alerts for threshold breaches (e.g., CPU &gt;90%, temp &gt;75°C). ◦\\n- Capacity Planning: Analyze trends to anticipate upgrades (e.g., disk nearing capacity). ◦\",\n",
       "  {'header_path': '/'}),\n",
       " ('## Software Administration\\n\\n- Question: How do you schedule and manage updates for software applications across multiple enterprise workstations? 1.\\n\\nAnswer: Use centralized   patch   management   tools   to   automate   deployment.   For   example, configure Windows Server Update Services (WSUS) or System Center Configuration Manager (SCCM) for Windows machines, and tools like Ansible, Puppet, or Chef for Linux environments. Create deployment groups (e.g., by department or operating system) and schedule updates during   off-peak   hours   to   minimize   disruption.   Always   test   updates   in   a   staging   or   pilot environment before full rollout. Maintain an inventory of installed software to track which applications need updating.\\n\\n- Question: What are the best practices for testing software before deployment in an enterprise environment? 2.\\n\\nAnswer: Effective testing includes multiple stages:\\n\\n- Staging Environment: Use a test network that mirrors production, including hardware and software configurations, to catch compatibility issues. 3.\\n- Automated Testing: Implement continuous integration/continuous deployment (CI/CD) pipelines with automated unit and integration tests. 4.\\n- User Acceptance Testing (UAT): Engage a group of end users to try new features and provide feedback. 5.\\n- Rollback Plan: Prepare a rollback procedure (backups, snapshots) in case the deployment causes issues. 6.\\n- Documentation: Maintain   clear   documentation   of   test   results   and   update   procedures   for auditing. 7.\\n- Question: How do you troubleshoot a program that fails to install on Windows due to a 8.\\n- missing .NET Framework dependency? Answer: First, identify the missing dependency by checking installation logs or Windows Event Viewer for errors. Then:\\n- Install Required .NET Version: Download and install the specific .NET Framework or .NET runtime required by the application. 9.\\n- Enable Windows Features: Go to \"Turn Windows features on or off\" and enable the needed .NET components on Windows. 10.\\n- Restart: Reboot the system after installing .NET. 11.\\n\\n- Re-run Installer: Run the application installer again, preferably as Administrator . 12.\\n- Verify Compatibility: Ensure the program supports your OS version; use compatibility mode if 13.\\n- needed. If the issue persists, check for a Microsoft support article or vendor documentation for specific fixes.\\n- Question: Describe the process of deploying a new enterprise application to multiple users. Answer: Deployment typically follows these steps: 14.\\n- Planning: Define requirements, target user groups, and dependencies. 15.\\n- Packaging: Create the application installer or deployment package (MSI, EXE, or script). 16.\\n- Testing: Install on test machines and resolve any compatibility issues. 17.\\n- Rollout: Use management tools (Group Policy, SCCM, Intune) to distribute the package to user workstations. 18.\\n- Verification: Check logs and user feedback to confirm successful installation. 19.\\n- Documentation: Update records with version, installation date, and deployment scope. 20.\\n- Support: Provide help for users if any issues arise post-deployment. 21.\\n- Question: How is Group Policy used in a Windows domain environment? 22.\\n- Answer: Group Policy Objects (GPOs) centrally configure settings for users and computers in\\n- Active Directory. By linking GPOs to Organizational Units (OUs), administrators enforce policies like password rules, software restrictions, mapped drives, and security configurations. GPOs have both computer and user sections. Computer policies (e.g., firewall rules, updates) apply at system startup, while user policies (e.g., desktop settings, folder redirection) apply at logon. This ensures consistent configurations and simplifies administration across the domain.\\n- Question: What is virtualization and how does it benefit an enterprise? Answer: Virtualization abstracts physical hardware into software-based virtual machines (VMs). 23.\\n- Benefits include:\\n- Resource Efficiency: Host multiple VMs on a single physical server , maximizing CPU and memory use. 24.\\n- Flexibility: Quickly create, clone, or migrate VMs for testing or scaling services. 25.\\n- Isolation: VMs are isolated from each other , improving security and stability (a VM crash doesn\\'t affect others). 26.\\n- Cost Savings: Reduce the number of physical servers, saving on hardware, power, and cooling. 27.\\n- Backup &amp; Recovery: Use snapshots or templates for easy backups and rapid recovery. 28.\\n- Legacy Support: Run outdated OS or applications in a VM while the host runs a modern OS. 29.\\n- Question: How do you manage configuration drift in a large fleet of servers? Answer: Prevent configuration drift by using automation and monitoring: 30.\\n- Configuration Management Tools: Use Ansible, Puppet, Chef, or Salt to define and enforce desired states. 31.\\n- Version Control: Store configuration scripts in Git to track changes. 32.\\n- Regular Audits: Schedule compliance scans to detect deviations (e.g., using Chef InSpec or OpenSCAP). 33.\\n\\n- Immutable Infrastructure: Deploy via immutable images or containers, redeploying rather than patching in-place. 34.\\n- Documentation: Log all manual changes in change management systems so nothing is done ad-hoc. 35.\\n- Question: What is a rollback plan, and why is it important during software updates? Answer: A rollback plan is a documented procedure to revert systems to a previous state if an update fails. It is important because it: 36.\\n- Minimizes Downtime: Restores service quickly when issues occur . 37.\\n- Ensures Data Integrity: Preserves recent data or configurations. 38.\\n- Reduces Risk: Allows teams to proceed with updates, knowing there\\'s a fallback. 39.\\n- Process: Typically includes taking backups (DB, configs), noting current versions, and having tested restore scripts. 40.\\n- Testing: Practice rollbacks periodically to ensure reliability. 41.\\n- Question: Explain the concept of a \\'staging environment\\' and its use in software deployment. Answer: A staging environment is a replica of production used for final testing before go-live. It includes similar hardware, software, and network configurations as production. Uses: 42.\\n- Validation: Detect issues that might not show up in dev (performance, integration). 43.\\n- Load Testing: Simulate user load or database size. 44.\\n- User Testing: Let power users verify new features. 45.\\n- Deployment Rehearsal: Practice deployment steps to catch procedural errors. Using staging reduces the risk of downtime or bugs in the live environment. 46.\\n- Question: How do you ensure software compliance (licensing) in an organization? Answer: Ensure compliance by: 47.\\n- Inventory: Maintain an up-to-date list of installed software (using SAM tools or scripts). ◦\\n- License Tracking: Record license keys, types, expiration dates centrally. ◦\\n- Audits: Regularly compare installed copies to purchased licenses. ◦\\n- Controlled Distribution: Use software deployment tools to prevent unauthorized installs. ◦\\n- User Training: Educate employees on approved software usage. ◦\\n- Tools: Use license management software that alerts when usage exceeds entitlements. Compliance avoids legal issues and unexpected costs. ◦',\n",
       "  {'header_path': '/'})]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "#QUERY =  'How do you ensure software compliance (licensing) in an organization '\n",
    "QUERY = \"What tools and metrics would you use to monitor server hardware performance?\"\n",
    "result = index.as_query_engine(llm=llm).query(QUERY)\n",
    "print(f\"Q: {QUERY}\\nA: {result.response.strip()}\\n\\nSources:\")\n",
    "display([(n.text, n.metadata) for n in result.source_nodes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.prompts import PromptTemplate\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "custom_prompt = PromptTemplate(\n",
    "    \"You are an expert IT assistant. Use the context below to answer the user's question. \"\n",
    "    \"If the answer in the context is written in bullet points or numbered lists, preserve that format in your answer.\\n\\n\"\n",
    "    \"Context:\\n{context_str}\\n\\n\"\n",
    "    \"Question: {query_str}\\n\"\n",
    "    \"Answer:\"\n",
    ")\n",
    "\n",
    "query_engine = RetrieverQueryEngine.from_args(\n",
    "    retriever=index.as_retriever(),\n",
    "    text_qa_template=custom_prompt,\n",
    "    llm=llm,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"device_name\": \"BranchRouter\",\n",
      "  \"device_type\": \"Router\",\n",
      "  \"interfaces\": [\n",
      "    {\n",
      "      \"name\": \"GigabitEthernet0/0\",\n",
      "      \"ip_address\": \"192.168.1.1\",\n",
      "      \"subnet_mask\": \"255.255.255.0\",\n",
      "      \"description\": \"LAN Interface\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"GigabitEthernet0/1\",\n",
      "      \"ip_address\": \"10.10.20.1\",\n",
      "      \"subnet_mask\": \"255.255.255.0\",\n",
      "      \"description\": \"Connection to Remote Network\"\n",
      "    }\n",
      "  ],\n",
      "  \"routing\": [\n",
      "    {\n",
      "      \"destination_network\": \"192.168.50.0/24\",\n",
      "      \"next_hop\": \"192.168.1.254\",\n",
      "      \"interface\": \"GigabitEthernet0/0\"\n",
      "    },\n",
      "    {\n",
      "      \"destination_network\": \"10.10.20.0/24\",\n",
      "      \"next_hop\": \"10.10.20.254\",\n",
      "      \"interface\": \"GigabitEthernet0/1\"\n",
      "    }\n",
      "  ],\n",
      "  \"firewall_rules\": [\n",
      "    {\n",
      "      \"source\": \"192.168.1.0/24\",\n",
      "      \"destination\": \"10.10.20.0/24\",\n",
      "      \"allow\": true,\n",
      "      \"description\": \"Allow traffic from LAN to Remote Network\"\n",
      "    },\n",
      "    {\n",
      "      \"source\": \"any\",\n",
      "      \"destination\": \"any\",\n",
      "      \"allow\": false,\n",
      "      \"description\": \"Deny all other traffic\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\" Provide a JSON-formatted example of a network device configuration for documentation\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying with: [{'role': 'system', 'content': 'You are an expert DevOps assistant.'}, {'role': 'user', 'content': 'What tools and metrics would you use to monitor server hardware performance?'}]\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "2 validation errors for QueryStartEvent\nquery.str\n  Input should be a valid string [type=string_type, input_value=[{'role': 'system', 'cont...hardware performance?'}], input_type=list]\n    For further information visit https://errors.pydantic.dev/2.11/v/string_type\nquery.QueryBundle\n  Input should be a dictionary or an instance of QueryBundle [type=dataclass_type, input_value=[{'role': 'system', 'cont...hardware performance?'}], input_type=list]\n    For further information visit https://errors.pydantic.dev/2.11/v/dataclass_type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m/Users/A200154990/lang/CSK/CSKBOT/dockling.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/A200154990/lang/CSK/CSKBOT/dockling.ipynb#X55sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m response \u001b[39m=\u001b[39m chat_engine\u001b[39m.\u001b[39;49mchat([\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/A200154990/lang/CSK/CSKBOT/dockling.ipynb#X55sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     {\u001b[39m\"\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39msystem\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mYou are an expert DevOps assistant.\u001b[39;49m\u001b[39m\"\u001b[39;49m},\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/A200154990/lang/CSK/CSKBOT/dockling.ipynb#X55sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     {\u001b[39m\"\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00mQUERY\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m}\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/A200154990/lang/CSK/CSKBOT/dockling.ipynb#X55sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m ])\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/A200154990/lang/CSK/CSKBOT/dockling.ipynb#X55sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(response\u001b[39m.\u001b[39mresponse)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages/llama_index_instrumentation/dispatcher.py:319\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    316\u001b[0m             _logger\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFailed to reset active_span_id: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    318\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 319\u001b[0m     result \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    320\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(result, asyncio\u001b[39m.\u001b[39mFuture):\n\u001b[1;32m    321\u001b[0m         \u001b[39m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[1;32m    322\u001b[0m         new_future \u001b[39m=\u001b[39m asyncio\u001b[39m.\u001b[39mensure_future(result)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages/llama_index/core/callbacks/utils.py:42\u001b[0m, in \u001b[0;36mtrace_method.<locals>.decorator.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m callback_manager \u001b[39m=\u001b[39m cast(CallbackManager, callback_manager)\n\u001b[1;32m     41\u001b[0m \u001b[39mwith\u001b[39;00m callback_manager\u001b[39m.\u001b[39mas_trace(trace_id):\n\u001b[0;32m---> 42\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages/llama_index/core/chat_engine/condense_question.py:197\u001b[0m, in \u001b[0;36mCondenseQuestionChatEngine.chat\u001b[0;34m(self, message, chat_history)\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_query_engine\u001b[39m.\u001b[39m_response_synthesizer\u001b[39m.\u001b[39m_streaming \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# Query with standalone question\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m query_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_query_engine\u001b[39m.\u001b[39;49mquery(condensed_question)\n\u001b[1;32m    199\u001b[0m \u001b[39m# NOTE: reset streaming flag\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_query_engine, RetrieverQueryEngine):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages/llama_index_instrumentation/dispatcher.py:319\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    316\u001b[0m             _logger\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFailed to reset active_span_id: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    318\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 319\u001b[0m     result \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    320\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(result, asyncio\u001b[39m.\u001b[39mFuture):\n\u001b[1;32m    321\u001b[0m         \u001b[39m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[1;32m    322\u001b[0m         new_future \u001b[39m=\u001b[39m asyncio\u001b[39m.\u001b[39mensure_future(result)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages/llama_index/core/base/base_query_engine.py:48\u001b[0m, in \u001b[0;36mBaseQueryEngine.query\u001b[0;34m(self, str_or_query_bundle)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39m@dispatcher\u001b[39m\u001b[39m.\u001b[39mspan\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mquery\u001b[39m(\u001b[39mself\u001b[39m, str_or_query_bundle: QueryType) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m RESPONSE_TYPE:\n\u001b[0;32m---> 48\u001b[0m     dispatcher\u001b[39m.\u001b[39mevent(QueryStartEvent(query\u001b[39m=\u001b[39;49mstr_or_query_bundle))\n\u001b[1;32m     49\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mas_trace(\u001b[39m\"\u001b[39m\u001b[39mquery\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     50\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(str_or_query_bundle, \u001b[39mstr\u001b[39m):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages/pydantic/main.py:253\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(self, **data)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[39m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[1;32m    252\u001b[0m __tracebackhide__ \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 253\u001b[0m validated_self \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__pydantic_validator__\u001b[39m.\u001b[39;49mvalidate_python(data, self_instance\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m)\n\u001b[1;32m    254\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m validated_self:\n\u001b[1;32m    255\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    256\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mA custom validator is returning a value other than `self`.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m    257\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt supported when validating via `__init__`.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    258\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m    259\u001b[0m         stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[1;32m    260\u001b[0m     )\n",
      "\u001b[0;31mValidationError\u001b[0m: 2 validation errors for QueryStartEvent\nquery.str\n  Input should be a valid string [type=string_type, input_value=[{'role': 'system', 'cont...hardware performance?'}], input_type=list]\n    For further information visit https://errors.pydantic.dev/2.11/v/string_type\nquery.QueryBundle\n  Input should be a dictionary or an instance of QueryBundle [type=dataclass_type, input_value=[{'role': 'system', 'cont...hardware performance?'}], input_type=list]\n    For further information visit https://errors.pydantic.dev/2.11/v/dataclass_type"
     ]
    }
   ],
   "source": [
    "response = chat_engine.chat([\n",
    "    {\"role\": \"system\", \"content\": \"You are an expert DevOps assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": f\"{QUERY}\"}\n",
    "])\n",
    "\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[NodeWithScore(node=TextNode(id_='7ebcb2d7-ea89-4a41-baac-60b58b1ee34e', embedding=None, metadata={'header_path': '/'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='7160dd5f-bb88-47b9-baaf-56d7d65fea07', node_type='4', metadata={}, hash='9f213321a00b85bf4435f6adc3dabeae885b47083972dd8ad7c7500b1e3c511a'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='5028152b-3b5d-44a4-99ce-8f5b51074a23', node_type='1', metadata={'header_path': '/'}, hash='257a0897aa55c114fee09468378d025c6963b9fe88f25e6c77f82833dec64e67'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='bda52f14-5a91-4dbc-9c44-fd0fb60b4ab1', node_type='1', metadata={'header_path': '/'}, hash='f08576f9e50072819042462c9305fdd2e1f540142bc22055d4f64b7cdc7978b3')}, metadata_template='{key}: {value}', metadata_separator='\\n', text=\"## Hardware Troubleshooting\\n\\n- Question: How do you diagnose a failing hard disk in a server? 1.\\n- Answer: Disk failures often cause slowdowns or errors. Troubleshooting steps:\\n- SMART Data: Use smartctl or a RAID controller utility to check SMART status (reallocated sectors, pending errors). 2.\\n- Noise Check: Listen for unusual sounds (clicking, beeping). 3.\\n- Log Review: Check system logs (Windows Event Viewer or /var/log/ ) for disk I/O errors. 4.\\n- Disk Check: Run file system checks (CHKDSK on Windows, fsck on Linux) to find bad sectors. 5.\\n- Backup Immediately: Copy critical data to backup storage at first sign of failure. 6.\\n\\n- Replace Disk: If   errors   persist,   replace the drive and rebuild/restore from backups or RAID parity. 7.\\n- Question: Describe the differences between RAID 0, 1, 5, and 10 in terms of redundancy and performance. 8.\\n\\nAnswer: RAID levels offer different trade-offs:\\n\\n- RAID 0 (Striping): No redundancy. Data is striped across disks for high performance, but if any disk fails, all data is lost. 9.\\n- RAID 1 (Mirroring): 1:1 data copy on two disks. High read performance (reads from either disk), safe storage (one disk can fail). 10.\\n- RAID 5 (Striping with Parity): Data is striped plus parity across 3+ disks. Can survive one disk failure. Good read speeds, slower writes (parity overhead). 11.\\n- RAID 10 (Mirrored Stripes): Combines RAID 1+0, requiring 4+ disks. Provides high performance and redundancy (can survive multiple failures if not in the same mirror). 12.\\n- Question: What are best practices for cooling and power management in a data center? Answer: Key best practices: 1.\\n- Hot/Cold Aisle: Arrange racks in hot and cold aisles; use containment to keep cold air in server intakes. 2.\\n- Clear Airflow: Avoid blocking vents; use blanking panels in empty rack spaces. 3.\\n- Temperature Monitoring: Install sensors to keep temperatures in recommended ranges (1827°C, 40-60% humidity). 4.\\n- Redundant Power: Use redundant power supplies and dual power feeds for critical equipment. 5.\\n- UPS &amp; Generators: Provide uninterruptible power supplies for clean shutdowns and generators for long outages. 6.\\n- Efficient Equipment: Choose high-efficiency power supplies (80 PLUS) and servers with power management. 7.\\n- Maintenance: Regularly clean air filters and ensure cooling units function properly. 8.\\n- Policies: Document and enforce environmental policies (e.g., max rack density, humidity levels). 9.\\n- Question: How do you safely replace a failed network switch in a live environment? Answer: Steps for minimal disruption: 10.\\n- Prepare Replacement: Pre-configure the new switch (IP, VLANs, ports) offline if possible. 11.\\n- Notify Stakeholders: Inform users of planned downtime; schedule during maintenance hours. 12.\\n- Backup Config: Save the old switch's configuration (if available) for reference. 13.\\n- Check Redundancy: Confirm any spanning tree or LACP settings won't cause a loop when the switch is removed. 14.\\n\\n|   RAID Level |   Min Disks | Fault Tolerance   | Performance               | Use Case                  |\\n|--------------|-------------|-------------------|---------------------------|---------------------------|\\n|            0 |           2 | None              | Very high                 | Temp data, caches         |\\n|            1 |           2 | 1 disk            | High reads, fair writes   | OS drives, critical data  |\\n|            5 |           3 | 1 disk            | Good reads, slower writes | File servers, backups     |\\n|           10 |           4 | 1 disk per mirror | Excellent                 | Databases, virtualization |\\n\\n- Swap Hardware: Disconnect the failed switch and immediately connect the new one with the same cables. 15.\\n- Power Up: Turn on the new switch and ensure power LEDs are normal. 16.\\n- Verify Connectivity: Test uplinks (ping core/router) and test devices on access ports. 17.\\n- Monitor: Watch for any abnormal behavior in the network; keep the old switch on standby until all is clear . 18.\\n- Question: What considerations are there when installing and configuring a new server rack? Answer: Important considerations: 19.\\n- Space &amp; Layout: Ensure adequate rack units (U) and plan equipment placement (heaviest at bottom). 20.\\n- Power Distribution: Install PDUs and ensure power circuits match equipment needs (voltage, phases). 21.\\n- Cooling: Check that air conditioning can handle the heat load; follow hot/cold aisle design. 22.\\n- Cable Management: Use proper cable trays, ties, and labels for network and power cables. 23.\\n- Grounding: Properly ground the rack and electrical systems to prevent static and surges. 24.\\n- Security: Lock the rack and control physical access. 25.\\n- Weight Rating: Verify the rack's load rating is not exceeded. 26.\\n- Documentation: Record rack diagram, device positions, and port mappings for inventory. 27.\\n- Question: What factors do you consider when selecting an enterprise-grade SSD for a server? Answer: Factors include: 28.\\n- Endurance: Look at Total Bytes Written (TBW); enterprise SSDs have higher endurance to handle heavy writes. 29.\\n- Performance: Consider IOPS and throughput (sequential vs random) based on workload (database, logging, etc.). 30.\\n- Interface: Choose appropriate interface (SATA, SAS, or NVMe PCIe). NVMe drives offer much higher performance if supported. 31.\\n- Capacity: Ensure sufficient capacity; also plan for RAID (mirroring or parity affects usable capacity). 32.\\n- Power-Loss Protection: Enterprise SSDs often include capacitors to preserve data in case of sudden power loss. 33.\\n- Security Features: Hardware encryption (SED) or secure erase features may be required. 34.\\n- Vendor Support &amp; Warranty: Check warranty period (often 3-5 years) and enterprise-grade support options. 35.\\n- Compatibility: Verify the server's hardware compatibility list and ensure firmware updates are available. 36.\\n- Question: How do you manage firmware and BIOS updates on network devices like routers and switches? 37.\\n- Answer: Managing updates:\\n- Inventory: Keep track of current firmware versions for each device. 38.\\n- Vendor Docs: Check release notes and compatibility charts to choose the correct firmware. 39.\\n- Backup Configs: Always back up device configurations before updating firmware. 40.\\n- Test: If possible, test updates on spare or lab devices first. 41.\\n- Schedule: Apply updates during maintenance windows to avoid service disruptions. 42.\\n\\n- Upgrade Process: Follow vendor procedures carefully (copy image file, verify checksum, reboot device). 43.\\n- Verify: After update, confirm firmware version and that settings were retained. Test functionality (e.g., ping neighbors, check routing). 44.\\n- Rollback Plan: Keep older firmware images available in case you need to revert. 45.\\n- Question: When a desktop computer fails to boot, which hardware components do you check first? 46.\\n- Answer: Check in order:\\n- Power Supply: Verify power cable and switch, listen for fan or drive spin. 47.\\n- POST Beeps/Codes: Listen to BIOS beep codes or check any error LEDs to identify faulty component. 48.\\n- Memory (RAM): Reseat RAM sticks; try booting with one module at a time. 49.\\n- Graphics/Display: Reseat GPU or try onboard video; check monitor power and cables. 50.\\n- Connections: Ensure motherboard power connectors (24-pin, 8-pin CPU) are secure. 51.\\n- CPU and Cooling: Confirm CPU cooler is attached properly; check for overheating (unlikely at power-on). 52.\\n- Peripherals: Disconnect USB drives, printers, etc., that could be interfering. 53.\\n- Swap Components: If possible, swap suspected bad parts (PSU, RAM, GPU) with known-good ones to isolate. 54.\\n- 55.\\n- Question: Explain the purpose of ECC RAM in servers.\\n- Answer: ECC   (Error-Correcting   Code)   RAM   detects   and   corrects   single-bit   memory   errors automatically. Its purpose:\\n- Data Integrity: Prevents data corruption by fixing bit flips (caused by electrical interference). 56.\\n- System Stability: Reduces crashes and blue screens due to memory errors. 57.\\n- Reliability: Crucial for servers running critical applications (databases, virtualization) where errors can have big impacts. 58.\\n- Cost vs Benefit: ECC modules cost a bit more and slightly slower due to parity calculations, but they are justified in enterprise for uptime and reliability. 59.\\n- Question: What tools and metrics would you use to monitor server hardware performance? Answer: Tools and metrics: 60.\\n- Monitoring Software: Use Nagios, Zabbix, Prometheus, or cloud tools (AWS CloudWatch, Azure Monitor). ◦\\n- Metrics: CPU usage, RAM usage, disk I/O (throughput/latency), disk space, network throughput, and temperatures. ◦\\n- SMART Data: Monitor disk health indicators (reallocated sectors, error rates) via SMART. ◦\\n- SNMP: Enable SNMP on devices to collect performance data. ◦\\n- Logs: Centralize logs (syslog, Event Viewer) to catch hardware errors (fan failures, power issues). ◦\\n- Dashboards/Alerts: Configure dashboards for real-time viewing and set alerts for threshold breaches (e.g., CPU &gt;90%, temp &gt;75°C). ◦\\n- Capacity Planning: Analyze trends to anticipate upgrades (e.g., disk nearing capacity). ◦\", mimetype='text/plain', start_char_idx=7590, end_char_idx=16744, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.7343480587005615),\n",
       " NodeWithScore(node=TextNode(id_='5028152b-3b5d-44a4-99ce-8f5b51074a23', embedding=None, metadata={'header_path': '/'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='7160dd5f-bb88-47b9-baaf-56d7d65fea07', node_type='4', metadata={}, hash='9f213321a00b85bf4435f6adc3dabeae885b47083972dd8ad7c7500b1e3c511a'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='78336bf3-3c73-4863-8afb-a6cc2c96b4ed', node_type='1', metadata={'header_path': '/'}, hash='00903630f5fb878a5551c9eccbdb975498e5aa58fe9ee86cdc4c1f81590a30db'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='7ebcb2d7-ea89-4a41-baac-60b58b1ee34e', node_type='1', metadata={'header_path': '/'}, hash='8d16b4751a93ab93a6be8e6229e3dcd84b8f2c52fb295186bc4b141cda7df835')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='## Software Administration\\n\\n- Question: How do you schedule and manage updates for software applications across multiple enterprise workstations? 1.\\n\\nAnswer: Use centralized   patch   management   tools   to   automate   deployment.   For   example, configure Windows Server Update Services (WSUS) or System Center Configuration Manager (SCCM) for Windows machines, and tools like Ansible, Puppet, or Chef for Linux environments. Create deployment groups (e.g., by department or operating system) and schedule updates during   off-peak   hours   to   minimize   disruption.   Always   test   updates   in   a   staging   or   pilot environment before full rollout. Maintain an inventory of installed software to track which applications need updating.\\n\\n- Question: What are the best practices for testing software before deployment in an enterprise environment? 2.\\n\\nAnswer: Effective testing includes multiple stages:\\n\\n- Staging Environment: Use a test network that mirrors production, including hardware and software configurations, to catch compatibility issues. 3.\\n- Automated Testing: Implement continuous integration/continuous deployment (CI/CD) pipelines with automated unit and integration tests. 4.\\n- User Acceptance Testing (UAT): Engage a group of end users to try new features and provide feedback. 5.\\n- Rollback Plan: Prepare a rollback procedure (backups, snapshots) in case the deployment causes issues. 6.\\n- Documentation: Maintain   clear   documentation   of   test   results   and   update   procedures   for auditing. 7.\\n- Question: How do you troubleshoot a program that fails to install on Windows due to a 8.\\n- missing .NET Framework dependency? Answer: First, identify the missing dependency by checking installation logs or Windows Event Viewer for errors. Then:\\n- Install Required .NET Version: Download and install the specific .NET Framework or .NET runtime required by the application. 9.\\n- Enable Windows Features: Go to \"Turn Windows features on or off\" and enable the needed .NET components on Windows. 10.\\n- Restart: Reboot the system after installing .NET. 11.\\n\\n- Re-run Installer: Run the application installer again, preferably as Administrator . 12.\\n- Verify Compatibility: Ensure the program supports your OS version; use compatibility mode if 13.\\n- needed. If the issue persists, check for a Microsoft support article or vendor documentation for specific fixes.\\n- Question: Describe the process of deploying a new enterprise application to multiple users. Answer: Deployment typically follows these steps: 14.\\n- Planning: Define requirements, target user groups, and dependencies. 15.\\n- Packaging: Create the application installer or deployment package (MSI, EXE, or script). 16.\\n- Testing: Install on test machines and resolve any compatibility issues. 17.\\n- Rollout: Use management tools (Group Policy, SCCM, Intune) to distribute the package to user workstations. 18.\\n- Verification: Check logs and user feedback to confirm successful installation. 19.\\n- Documentation: Update records with version, installation date, and deployment scope. 20.\\n- Support: Provide help for users if any issues arise post-deployment. 21.\\n- Question: How is Group Policy used in a Windows domain environment? 22.\\n- Answer: Group Policy Objects (GPOs) centrally configure settings for users and computers in\\n- Active Directory. By linking GPOs to Organizational Units (OUs), administrators enforce policies like password rules, software restrictions, mapped drives, and security configurations. GPOs have both computer and user sections. Computer policies (e.g., firewall rules, updates) apply at system startup, while user policies (e.g., desktop settings, folder redirection) apply at logon. This ensures consistent configurations and simplifies administration across the domain.\\n- Question: What is virtualization and how does it benefit an enterprise? Answer: Virtualization abstracts physical hardware into software-based virtual machines (VMs). 23.\\n- Benefits include:\\n- Resource Efficiency: Host multiple VMs on a single physical server , maximizing CPU and memory use. 24.\\n- Flexibility: Quickly create, clone, or migrate VMs for testing or scaling services. 25.\\n- Isolation: VMs are isolated from each other , improving security and stability (a VM crash doesn\\'t affect others). 26.\\n- Cost Savings: Reduce the number of physical servers, saving on hardware, power, and cooling. 27.\\n- Backup &amp; Recovery: Use snapshots or templates for easy backups and rapid recovery. 28.\\n- Legacy Support: Run outdated OS or applications in a VM while the host runs a modern OS. 29.\\n- Question: How do you manage configuration drift in a large fleet of servers? Answer: Prevent configuration drift by using automation and monitoring: 30.\\n- Configuration Management Tools: Use Ansible, Puppet, Chef, or Salt to define and enforce desired states. 31.\\n- Version Control: Store configuration scripts in Git to track changes. 32.\\n- Regular Audits: Schedule compliance scans to detect deviations (e.g., using Chef InSpec or OpenSCAP). 33.\\n\\n- Immutable Infrastructure: Deploy via immutable images or containers, redeploying rather than patching in-place. 34.\\n- Documentation: Log all manual changes in change management systems so nothing is done ad-hoc. 35.\\n- Question: What is a rollback plan, and why is it important during software updates? Answer: A rollback plan is a documented procedure to revert systems to a previous state if an update fails. It is important because it: 36.\\n- Minimizes Downtime: Restores service quickly when issues occur . 37.\\n- Ensures Data Integrity: Preserves recent data or configurations. 38.\\n- Reduces Risk: Allows teams to proceed with updates, knowing there\\'s a fallback. 39.\\n- Process: Typically includes taking backups (DB, configs), noting current versions, and having tested restore scripts. 40.\\n- Testing: Practice rollbacks periodically to ensure reliability. 41.\\n- Question: Explain the concept of a \\'staging environment\\' and its use in software deployment. Answer: A staging environment is a replica of production used for final testing before go-live. It includes similar hardware, software, and network configurations as production. Uses: 42.\\n- Validation: Detect issues that might not show up in dev (performance, integration). 43.\\n- Load Testing: Simulate user load or database size. 44.\\n- User Testing: Let power users verify new features. 45.\\n- Deployment Rehearsal: Practice deployment steps to catch procedural errors. Using staging reduces the risk of downtime or bugs in the live environment. 46.\\n- Question: How do you ensure software compliance (licensing) in an organization? Answer: Ensure compliance by: 47.\\n- Inventory: Maintain an up-to-date list of installed software (using SAM tools or scripts). ◦\\n- License Tracking: Record license keys, types, expiration dates centrally. ◦\\n- Audits: Regularly compare installed copies to purchased licenses. ◦\\n- Controlled Distribution: Use software deployment tools to prevent unauthorized installs. ◦\\n- User Training: Educate employees on approved software usage. ◦\\n- Tools: Use license management software that alerts when usage exceeds entitlements. Compliance avoids legal issues and unexpected costs. ◦', mimetype='text/plain', start_char_idx=369, end_char_idx=7588, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.6740434169769287)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.source_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "llama_index.core.indices.vector_store.base.VectorStoreIndex"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "client = InferenceClient(token=os.getenv(\"HF_TOKEN\"))\n",
    "response = client.text_generation(\n",
    "    prompt=\"How old was albert einstein when he died?\",\n",
    "    model=\"google/flan-t5-base\",\n",
    "    max_new_tokens=100\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "HfHubHTTPError",
     "evalue": "404 Client Error: Not Found for url: https://router.huggingface.co/hf-inference/models/HuggingFaceH4/zephyr-7b-beta/v1/chat/completions (Request ID: Root=1-6867631b-73cd515207d00dd6605e66ac;ee8bba0d-649b-463a-8593-7644c55ef6fe)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:409\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 409\u001b[0m     response\u001b[39m.\u001b[39;49mraise_for_status()\n\u001b[1;32m    410\u001b[0m \u001b[39mexcept\u001b[39;00m HTTPError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[39mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[39mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://router.huggingface.co/hf-inference/models/HuggingFaceH4/zephyr-7b-beta/v1/chat/completions",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/A200154990/lang/CSK/CSKBOT/dockling.ipynb Cell 17\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/A200154990/lang/CSK/CSKBOT/dockling.ipynb#X26sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m GEN_MODEL \u001b[39m=\u001b[39m HuggingFaceInferenceAPI(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/A200154990/lang/CSK/CSKBOT/dockling.ipynb#X26sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     token\u001b[39m=\u001b[39mos\u001b[39m.\u001b[39mgetenv(\u001b[39m\"\u001b[39m\u001b[39mHF_TOKEN\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/A200154990/lang/CSK/CSKBOT/dockling.ipynb#X26sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     model_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mHuggingFaceH4/zephyr-7b-beta\u001b[39m\u001b[39m\"\u001b[39m,)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/A200154990/lang/CSK/CSKBOT/dockling.ipynb#X26sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m QUERY \u001b[39m=\u001b[39m  \u001b[39m'\u001b[39m\u001b[39mHow old was albert einstein when he died?\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/A200154990/lang/CSK/CSKBOT/dockling.ipynb#X26sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m result \u001b[39m=\u001b[39m index\u001b[39m.\u001b[39;49mas_query_engine(llm\u001b[39m=\u001b[39;49mGEN_MODEL)\u001b[39m.\u001b[39;49mquery(QUERY)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/A200154990/lang/CSK/CSKBOT/dockling.ipynb#X26sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mQ: \u001b[39m\u001b[39m{\u001b[39;00mQUERY\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mA: \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mstrip()\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mSources:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/A200154990/lang/CSK/CSKBOT/dockling.ipynb#X26sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m display([(n\u001b[39m.\u001b[39mtext, n\u001b[39m.\u001b[39mmetadata) \u001b[39mfor\u001b[39;00m n \u001b[39min\u001b[39;00m result\u001b[39m.\u001b[39msource_nodes])\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages/llama_index_instrumentation/dispatcher.py:319\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    316\u001b[0m             _logger\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFailed to reset active_span_id: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    318\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 319\u001b[0m     result \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    320\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(result, asyncio\u001b[39m.\u001b[39mFuture):\n\u001b[1;32m    321\u001b[0m         \u001b[39m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[1;32m    322\u001b[0m         new_future \u001b[39m=\u001b[39m asyncio\u001b[39m.\u001b[39mensure_future(result)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages/llama_index/core/base/base_query_engine.py:52\u001b[0m, in \u001b[0;36mBaseQueryEngine.query\u001b[0;34m(self, str_or_query_bundle)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(str_or_query_bundle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m     51\u001b[0m         str_or_query_bundle \u001b[39m=\u001b[39m QueryBundle(str_or_query_bundle)\n\u001b[0;32m---> 52\u001b[0m     query_result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_query(str_or_query_bundle)\n\u001b[1;32m     53\u001b[0m dispatcher\u001b[39m.\u001b[39mevent(\n\u001b[1;32m     54\u001b[0m     QueryEndEvent(query\u001b[39m=\u001b[39mstr_or_query_bundle, response\u001b[39m=\u001b[39mquery_result)\n\u001b[1;32m     55\u001b[0m )\n\u001b[1;32m     56\u001b[0m \u001b[39mreturn\u001b[39;00m query_result\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages/llama_index_instrumentation/dispatcher.py:319\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    316\u001b[0m             _logger\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFailed to reset active_span_id: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    318\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 319\u001b[0m     result \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    320\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(result, asyncio\u001b[39m.\u001b[39mFuture):\n\u001b[1;32m    321\u001b[0m         \u001b[39m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[1;32m    322\u001b[0m         new_future \u001b[39m=\u001b[39m asyncio\u001b[39m.\u001b[39mensure_future(result)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages/llama_index/core/query_engine/retriever_query_engine.py:183\u001b[0m, in \u001b[0;36mRetrieverQueryEngine._query\u001b[0;34m(self, query_bundle)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mevent(\n\u001b[1;32m    180\u001b[0m     CBEventType\u001b[39m.\u001b[39mQUERY, payload\u001b[39m=\u001b[39m{EventPayload\u001b[39m.\u001b[39mQUERY_STR: query_bundle\u001b[39m.\u001b[39mquery_str}\n\u001b[1;32m    181\u001b[0m ) \u001b[39mas\u001b[39;00m query_event:\n\u001b[1;32m    182\u001b[0m     nodes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mretrieve(query_bundle)\n\u001b[0;32m--> 183\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_response_synthesizer\u001b[39m.\u001b[39;49msynthesize(\n\u001b[1;32m    184\u001b[0m         query\u001b[39m=\u001b[39;49mquery_bundle,\n\u001b[1;32m    185\u001b[0m         nodes\u001b[39m=\u001b[39;49mnodes,\n\u001b[1;32m    186\u001b[0m     )\n\u001b[1;32m    187\u001b[0m     query_event\u001b[39m.\u001b[39mon_end(payload\u001b[39m=\u001b[39m{EventPayload\u001b[39m.\u001b[39mRESPONSE: response})\n\u001b[1;32m    189\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages/llama_index_instrumentation/dispatcher.py:319\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    316\u001b[0m             _logger\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFailed to reset active_span_id: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    318\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 319\u001b[0m     result \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    320\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(result, asyncio\u001b[39m.\u001b[39mFuture):\n\u001b[1;32m    321\u001b[0m         \u001b[39m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[1;32m    322\u001b[0m         new_future \u001b[39m=\u001b[39m asyncio\u001b[39m.\u001b[39mensure_future(result)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages/llama_index/core/response_synthesizers/base.py:242\u001b[0m, in \u001b[0;36mBaseSynthesizer.synthesize\u001b[0;34m(self, query, nodes, additional_source_nodes, **response_kwargs)\u001b[0m\n\u001b[1;32m    236\u001b[0m     query \u001b[39m=\u001b[39m QueryBundle(query_str\u001b[39m=\u001b[39mquery)\n\u001b[1;32m    238\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_callback_manager\u001b[39m.\u001b[39mevent(\n\u001b[1;32m    239\u001b[0m     CBEventType\u001b[39m.\u001b[39mSYNTHESIZE,\n\u001b[1;32m    240\u001b[0m     payload\u001b[39m=\u001b[39m{EventPayload\u001b[39m.\u001b[39mQUERY_STR: query\u001b[39m.\u001b[39mquery_str},\n\u001b[1;32m    241\u001b[0m ) \u001b[39mas\u001b[39;00m event:\n\u001b[0;32m--> 242\u001b[0m     response_str \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_response(\n\u001b[1;32m    243\u001b[0m         query_str\u001b[39m=\u001b[39;49mquery\u001b[39m.\u001b[39;49mquery_str,\n\u001b[1;32m    244\u001b[0m         text_chunks\u001b[39m=\u001b[39;49m[\n\u001b[1;32m    245\u001b[0m             n\u001b[39m.\u001b[39;49mnode\u001b[39m.\u001b[39;49mget_content(metadata_mode\u001b[39m=\u001b[39;49mMetadataMode\u001b[39m.\u001b[39;49mLLM) \u001b[39mfor\u001b[39;49;00m n \u001b[39min\u001b[39;49;00m nodes\n\u001b[1;32m    246\u001b[0m         ],\n\u001b[1;32m    247\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kwargs,\n\u001b[1;32m    248\u001b[0m     )\n\u001b[1;32m    250\u001b[0m     additional_source_nodes \u001b[39m=\u001b[39m additional_source_nodes \u001b[39mor\u001b[39;00m []\n\u001b[1;32m    251\u001b[0m     source_nodes \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(nodes) \u001b[39m+\u001b[39m \u001b[39mlist\u001b[39m(additional_source_nodes)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages/llama_index_instrumentation/dispatcher.py:319\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    316\u001b[0m             _logger\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFailed to reset active_span_id: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    318\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 319\u001b[0m     result \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    320\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(result, asyncio\u001b[39m.\u001b[39mFuture):\n\u001b[1;32m    321\u001b[0m         \u001b[39m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[1;32m    322\u001b[0m         new_future \u001b[39m=\u001b[39m asyncio\u001b[39m.\u001b[39mensure_future(result)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py:43\u001b[0m, in \u001b[0;36mCompactAndRefine.get_response\u001b[0;34m(self, query_str, text_chunks, prev_response, **response_kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[39m# use prompt helper to fix compact text_chunks under the prompt limitation\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[39m# TODO: This is a temporary fix - reason it's temporary is that\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[39m# the refine template does not account for size of previous answer.\u001b[39;00m\n\u001b[1;32m     42\u001b[0m new_texts \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_compact_text_chunks(query_str, text_chunks)\n\u001b[0;32m---> 43\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mget_response(\n\u001b[1;32m     44\u001b[0m     query_str\u001b[39m=\u001b[39;49mquery_str,\n\u001b[1;32m     45\u001b[0m     text_chunks\u001b[39m=\u001b[39;49mnew_texts,\n\u001b[1;32m     46\u001b[0m     prev_response\u001b[39m=\u001b[39;49mprev_response,\n\u001b[1;32m     47\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kwargs,\n\u001b[1;32m     48\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages/llama_index_instrumentation/dispatcher.py:319\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    316\u001b[0m             _logger\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFailed to reset active_span_id: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    318\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 319\u001b[0m     result \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    320\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(result, asyncio\u001b[39m.\u001b[39mFuture):\n\u001b[1;32m    321\u001b[0m         \u001b[39m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[1;32m    322\u001b[0m         new_future \u001b[39m=\u001b[39m asyncio\u001b[39m.\u001b[39mensure_future(result)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages/llama_index/core/response_synthesizers/refine.py:179\u001b[0m, in \u001b[0;36mRefine.get_response\u001b[0;34m(self, query_str, text_chunks, prev_response, **response_kwargs)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[39mfor\u001b[39;00m text_chunk \u001b[39min\u001b[39;00m text_chunks:\n\u001b[1;32m    176\u001b[0m     \u001b[39mif\u001b[39;00m prev_response \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m         \u001b[39m# if this is the first chunk, and text chunk already\u001b[39;00m\n\u001b[1;32m    178\u001b[0m         \u001b[39m# is an answer, then return it\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m         response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_give_response_single(\n\u001b[1;32m    180\u001b[0m             query_str, text_chunk, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kwargs\n\u001b[1;32m    181\u001b[0m         )\n\u001b[1;32m    182\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    183\u001b[0m         \u001b[39m# refine response if possible\u001b[39;00m\n\u001b[1;32m    184\u001b[0m         response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_refine_response_single(\n\u001b[1;32m    185\u001b[0m             prev_response, query_str, text_chunk, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mresponse_kwargs\n\u001b[1;32m    186\u001b[0m         )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages/llama_index/core/response_synthesizers/refine.py:241\u001b[0m, in \u001b[0;36mRefine._give_response_single\u001b[0;34m(self, query_str, text_chunk, **response_kwargs)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[39mif\u001b[39;00m response \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_streaming:\n\u001b[1;32m    238\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    239\u001b[0m         structured_response \u001b[39m=\u001b[39m cast(\n\u001b[1;32m    240\u001b[0m             StructuredRefineResponse,\n\u001b[0;32m--> 241\u001b[0m             program(\n\u001b[1;32m    242\u001b[0m                 context_str\u001b[39m=\u001b[39;49mcur_text_chunk,\n\u001b[1;32m    243\u001b[0m                 \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kwargs,\n\u001b[1;32m    244\u001b[0m             ),\n\u001b[1;32m    245\u001b[0m         )\n\u001b[1;32m    246\u001b[0m         query_satisfied \u001b[39m=\u001b[39m structured_response\u001b[39m.\u001b[39mquery_satisfied\n\u001b[1;32m    247\u001b[0m         \u001b[39mif\u001b[39;00m query_satisfied:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages/llama_index_instrumentation/dispatcher.py:319\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    316\u001b[0m             _logger\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFailed to reset active_span_id: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    318\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 319\u001b[0m     result \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    320\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(result, asyncio\u001b[39m.\u001b[39mFuture):\n\u001b[1;32m    321\u001b[0m         \u001b[39m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[1;32m    322\u001b[0m         new_future \u001b[39m=\u001b[39m asyncio\u001b[39m.\u001b[39mensure_future(result)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages/llama_index/core/response_synthesizers/refine.py:85\u001b[0m, in \u001b[0;36mDefaultRefineProgram.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m     83\u001b[0m         answer \u001b[39m=\u001b[39m answer\u001b[39m.\u001b[39mmodel_dump_json()\n\u001b[1;32m     84\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 85\u001b[0m     answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_llm\u001b[39m.\u001b[39;49mpredict(\n\u001b[1;32m     86\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_prompt,\n\u001b[1;32m     87\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds,\n\u001b[1;32m     88\u001b[0m     )\n\u001b[1;32m     89\u001b[0m \u001b[39mreturn\u001b[39;00m StructuredRefineResponse(answer\u001b[39m=\u001b[39manswer, query_satisfied\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages/llama_index_instrumentation/dispatcher.py:319\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    316\u001b[0m             _logger\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFailed to reset active_span_id: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    318\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 319\u001b[0m     result \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    320\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(result, asyncio\u001b[39m.\u001b[39mFuture):\n\u001b[1;32m    321\u001b[0m         \u001b[39m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[1;32m    322\u001b[0m         new_future \u001b[39m=\u001b[39m asyncio\u001b[39m.\u001b[39mensure_future(result)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages/llama_index/core/llms/llm.py:641\u001b[0m, in \u001b[0;36mLLM.predict\u001b[0;34m(self, prompt, **prompt_args)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetadata\u001b[39m.\u001b[39mis_chat_model:\n\u001b[1;32m    640\u001b[0m     messages \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_messages(prompt, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mprompt_args)\n\u001b[0;32m--> 641\u001b[0m     chat_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchat(messages)\n\u001b[1;32m    642\u001b[0m     output \u001b[39m=\u001b[39m chat_response\u001b[39m.\u001b[39mmessage\u001b[39m.\u001b[39mcontent \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    643\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages/llama_index_instrumentation/dispatcher.py:319\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    316\u001b[0m             _logger\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFailed to reset active_span_id: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    318\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 319\u001b[0m     result \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    320\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(result, asyncio\u001b[39m.\u001b[39mFuture):\n\u001b[1;32m    321\u001b[0m         \u001b[39m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[1;32m    322\u001b[0m         new_future \u001b[39m=\u001b[39m asyncio\u001b[39m.\u001b[39mensure_future(result)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages/llama_index/llms/huggingface_api/base.py:287\u001b[0m, in \u001b[0;36mHuggingFaceInferenceAPI.chat\u001b[0;34m(self, messages, **kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtask \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mconversational\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtask \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    285\u001b[0m     model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_model_kwargs(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 287\u001b[0m     output: ChatCompletionOutput \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sync_client\u001b[39m.\u001b[39;49mchat_completion(\n\u001b[1;32m    288\u001b[0m         messages\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_to_huggingface_messages(messages),\n\u001b[1;32m    289\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m    290\u001b[0m     )\n\u001b[1;32m    292\u001b[0m     content \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mchoices[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mmessage\u001b[39m.\u001b[39mcontent \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    293\u001b[0m     tool_calls \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mchoices[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mmessage\u001b[39m.\u001b[39mtool_calls \u001b[39mor\u001b[39;00m []\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages/huggingface_hub/inference/_client.py:924\u001b[0m, in \u001b[0;36mInferenceClient.chat_completion\u001b[0;34m(self, messages, model, stream, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream_options, temperature, tool_choice, tool_prompt, tools, top_logprobs, top_p, extra_body)\u001b[0m\n\u001b[1;32m    896\u001b[0m parameters \u001b[39m=\u001b[39m {\n\u001b[1;32m    897\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m: payload_model,\n\u001b[1;32m    898\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mfrequency_penalty\u001b[39m\u001b[39m\"\u001b[39m: frequency_penalty,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    915\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m(extra_body \u001b[39mor\u001b[39;00m {}),\n\u001b[1;32m    916\u001b[0m }\n\u001b[1;32m    917\u001b[0m request_parameters \u001b[39m=\u001b[39m provider_helper\u001b[39m.\u001b[39mprepare_request(\n\u001b[1;32m    918\u001b[0m     inputs\u001b[39m=\u001b[39mmessages,\n\u001b[1;32m    919\u001b[0m     parameters\u001b[39m=\u001b[39mparameters,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    922\u001b[0m     api_key\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtoken,\n\u001b[1;32m    923\u001b[0m )\n\u001b[0;32m--> 924\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inner_post(request_parameters, stream\u001b[39m=\u001b[39;49mstream)\n\u001b[1;32m    926\u001b[0m \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    927\u001b[0m     \u001b[39mreturn\u001b[39;00m _stream_chat_completion_response(data)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages/huggingface_hub/inference/_client.py:280\u001b[0m, in \u001b[0;36mInferenceClient._inner_post\u001b[0;34m(self, request_parameters, stream)\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[39mraise\u001b[39;00m InferenceTimeoutError(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInference call timed out: \u001b[39m\u001b[39m{\u001b[39;00mrequest_parameters\u001b[39m.\u001b[39murl\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39merror\u001b[39;00m  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m    279\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 280\u001b[0m     hf_raise_for_status(response)\n\u001b[1;32m    281\u001b[0m     \u001b[39mreturn\u001b[39;00m response\u001b[39m.\u001b[39miter_lines() \u001b[39mif\u001b[39;00m stream \u001b[39melse\u001b[39;00m response\u001b[39m.\u001b[39mcontent\n\u001b[1;32m    282\u001b[0m \u001b[39mexcept\u001b[39;00m HTTPError \u001b[39mas\u001b[39;00m error:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:482\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    478\u001b[0m     \u001b[39mraise\u001b[39;00m _format(HfHubHTTPError, message, response) \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39me\u001b[39;00m\n\u001b[1;32m    480\u001b[0m \u001b[39m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[1;32m    481\u001b[0m \u001b[39m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[0;32m--> 482\u001b[0m \u001b[39mraise\u001b[39;00m _format(HfHubHTTPError, \u001b[39mstr\u001b[39m(e), response) \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39me\u001b[39;00m\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://router.huggingface.co/hf-inference/models/HuggingFaceH4/zephyr-7b-beta/v1/chat/completions (Request ID: Root=1-6867631b-73cd515207d00dd6605e66ac;ee8bba0d-649b-463a-8593-7644c55ef6fe)"
     ]
    }
   ],
   "source": [
    "\n",
    "GEN_MODEL = HuggingFaceInferenceAPI(\n",
    "    token=os.getenv(\"HF_TOKEN\"),\n",
    "    model_name=\"HuggingFaceH4/zephyr-7b-beta\",)\n",
    "\n",
    "QUERY =  'How old was albert einstein when he died?'\n",
    "result = index.as_query_engine(llm=GEN_MODEL).query(QUERY)\n",
    "print(f\"Q: {QUERY}\\nA: {result.response.strip()}\\n\\nSources:\")\n",
    "display([(n.text, n.metadata) for n in result.source_nodes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Hugging Face Inference API Fix ===\n",
      "\n",
      "🔧 Trying different solutions...\n",
      "\n",
      "\n",
      "==================================================\n",
      "🧪 Solution 1: InferenceClient\n",
      "==================================================\n",
      "Trying model: mistralai/Mixtral-8x7B-Instruct-v0.1\n",
      "Q: How old was albert einstein when he died?\n",
      "A:  Albert Einstein died at the age of 76. He lived a long and impactful life, contributing significantly to the field of theoretical physics and changing the way we understand time, space, energy, and matter. Einstein is best known for his theory of relativity, which includes the famous equation E=mc², demonstrating the equivalence of energy (E) and mass (m) when coupled with the speed of light (c). Despite his many accomplishments and accolades, Einstein remained humble and continued to be curious and passionate about the mysteries of the universe until the end of his life.\n",
      "✅ Solution 1: InferenceClient worked!\n",
      "\n",
      "==================================================\n",
      "💡 Tips:\n",
      "1. Some models require paid HF Pro subscription\n",
      "2. Try smaller models like 'google/flan-t5-base'\n",
      "3. Check model availability at https://huggingface.co/models\n",
      "4. Consider using OpenAI API as alternative\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from huggingface_hub import InferenceClient\n",
    "import requests\n",
    "\n",
    "# Solution 1: Use InferenceClient (Recommended)\n",
    "def solution_1_inference_client():\n",
    "    \"\"\"Using the official InferenceClient\"\"\"\n",
    "    try:\n",
    "        client = InferenceClient(token=os.getenv(\"HF_TOKEN\"))\n",
    "        \n",
    "        # Try different model names that are known to work\n",
    "        models_to_try = [\n",
    "            \"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "            \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "            \"microsoft/DialoGPT-medium\",\n",
    "            \"google/flan-t5-base\",\n",
    "            \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "        ]\n",
    "        \n",
    "        QUERY = 'How old was albert einstein when he died?'\n",
    "        \n",
    "        for model_name in models_to_try:\n",
    "            try:\n",
    "                print(f\"Trying model: {model_name}\")\n",
    "                \n",
    "                # For chat models\n",
    "                if \"instruct\" in model_name.lower() or \"chat\" in model_name.lower():\n",
    "                    messages = [{\"role\": \"user\", \"content\": QUERY}]\n",
    "                    response = client.chat_completion(\n",
    "                        messages=messages,\n",
    "                        model=model_name,\n",
    "                        max_tokens=500\n",
    "                    )\n",
    "                    result = response.choices[0].message.content\n",
    "                else:\n",
    "                    # For text generation models\n",
    "                    response = client.text_generation(\n",
    "                        prompt=QUERY,\n",
    "                        model=model_name,\n",
    "                        max_new_tokens=500\n",
    "                    )\n",
    "                    result = response\n",
    "                \n",
    "                print(f\"Q: {QUERY}\")\n",
    "                print(f\"A: {result}\")\n",
    "                return result\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error with {model_name}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(\"All models failed to respond\")\n",
    "        return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error setting up InferenceClient: {e}\")\n",
    "        return None\n",
    "\n",
    "# Solution 2: Direct API call with requests\n",
    "def solution_2_direct_api():\n",
    "    \"\"\"Direct API call to HuggingFace\"\"\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {os.getenv('HF_TOKEN')}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    # Models that are more likely to work with the inference API\n",
    "    models_to_try = [\n",
    "        \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "        \"microsoft/DialoGPT-medium\",\n",
    "        \"google/flan-t5-base\",\n",
    "        \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "    ]\n",
    "    \n",
    "    QUERY = 'How old was albert einstein when he died?'\n",
    "    \n",
    "    for model_name in models_to_try:\n",
    "        try:\n",
    "            print(f\"Trying model: {model_name}\")\n",
    "            \n",
    "            url = f\"https://api-inference.huggingface.co/models/{model_name}\"\n",
    "            \n",
    "            payload = {\n",
    "                \"inputs\": QUERY,\n",
    "                \"parameters\": {\n",
    "                    \"max_new_tokens\": 500,\n",
    "                    \"temperature\": 0.7\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            response = requests.post(url, headers=headers, json=payload)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                result = response.json()\n",
    "                if isinstance(result, list) and len(result) > 0:\n",
    "                    answer = result[0].get('generated_text', '')\n",
    "                    print(f\"Q: {QUERY}\")\n",
    "                    print(f\"A: {answer}\")\n",
    "                    return answer\n",
    "                else:\n",
    "                    print(f\"Unexpected response format: {result}\")\n",
    "            else:\n",
    "                print(f\"Error {response.status_code}: {response.text}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error with {model_name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(\"All models failed to respond\")\n",
    "    return None\n",
    "\n",
    "# Solution 3: Using a working model for your LlamaIndex setup\n",
    "def solution_3_llamaindex_compatible():\n",
    "    \"\"\"Code that should work with your LlamaIndex setup\"\"\"\n",
    "    from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "    \n",
    "    # Alternative approach using HuggingFaceLLM instead of HuggingFaceInferenceAPI\n",
    "    try:\n",
    "        # Use a model that's known to work\n",
    "        llm = HuggingFaceLLM(\n",
    "            model_name=\"microsoft/DialoGPT-medium\",\n",
    "            tokenizer_name=\"microsoft/DialoGPT-medium\",\n",
    "            query_wrapper_prompt=\"<|User|>: {query_str}\\n<|Assistant|>: \",\n",
    "            context_window=2048,\n",
    "            max_new_tokens=512,\n",
    "            model_kwargs={\"torch_dtype\": \"auto\"},\n",
    "            tokenizer_kwargs={},\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "        \n",
    "        QUERY = 'How old was albert einstein when he died?'\n",
    "        \n",
    "        # If you have an index setup\n",
    "        # result = index.as_query_engine(llm=llm).query(QUERY)\n",
    "        # print(f\"Q: {QUERY}\\nA: {result.response.strip()}\\n\\nSources:\")\n",
    "        # display([(n.text, n.metadata) for n in result.source_nodes])\n",
    "        \n",
    "        # For direct testing without index\n",
    "        response = llm.complete(QUERY)\n",
    "        print(f\"Q: {QUERY}\")\n",
    "        print(f\"A: {response.text}\")\n",
    "        \n",
    "        return response.text\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error with HuggingFaceLLM: {e}\")\n",
    "        return None\n",
    "\n",
    "# Solution 4: Simple working example with a free model\n",
    "def solution_4_simple_working():\n",
    "    \"\"\"Simple working example with a free model\"\"\"\n",
    "    from huggingface_hub import InferenceClient\n",
    "    \n",
    "    try:\n",
    "        client = InferenceClient(token=os.getenv(\"HF_TOKEN\"))\n",
    "        \n",
    "        # Use a model that's definitely available\n",
    "        model_name = \"google/flan-t5-base\"\n",
    "        QUERY = 'How old was albert einstein when he died?'\n",
    "        \n",
    "        response = client.text_generation(\n",
    "            prompt=f\"Question: {QUERY}\\nAnswer:\",\n",
    "            model=model_name,\n",
    "            max_new_tokens=100\n",
    "        )\n",
    "        \n",
    "        print(f\"Q: {QUERY}\")\n",
    "        print(f\"A: {response}\")\n",
    "        return response\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=== Hugging Face Inference API Fix ===\\n\")\n",
    "    \n",
    "    # Check if HF_TOKEN is set\n",
    "    if not os.getenv(\"HF_TOKEN\"):\n",
    "        print(\"❌ HF_TOKEN environment variable not set!\")\n",
    "        print(\"Please set it with: export HF_TOKEN='your_token_here'\")\n",
    "        exit(1)\n",
    "    \n",
    "    print(\"🔧 Trying different solutions...\\n\")\n",
    "    \n",
    "    # Try solutions in order\n",
    "    solutions = [\n",
    "        (\"Solution 1: InferenceClient\", solution_1_inference_client),\n",
    "        (\"Solution 2: Direct API\", solution_2_direct_api),\n",
    "        (\"Solution 3: LlamaIndex Compatible\", solution_3_llamaindex_compatible),\n",
    "        (\"Solution 4: Simple Working\", solution_4_simple_working)\n",
    "    ]\n",
    "    \n",
    "    for name, func in solutions:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"🧪 {name}\")\n",
    "        print('='*50)\n",
    "        \n",
    "        result = func()\n",
    "        if result:\n",
    "            print(f\"✅ {name} worked!\")\n",
    "            break\n",
    "        else:\n",
    "            print(f\"❌ {name} failed\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"💡 Tips:\")\n",
    "    print(\"1. Some models require paid HF Pro subscription\")\n",
    "    print(\"2. Try smaller models like 'google/flan-t5-base'\")\n",
    "    print(\"3. Check model availability at https://huggingface.co/models\")\n",
    "    print(\"4. Consider using OpenAI API as alternative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
