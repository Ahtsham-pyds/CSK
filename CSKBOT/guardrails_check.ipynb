{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from tempfile import mkdtemp\n",
    "from warnings import filterwarnings\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core import StorageContext, VectorStoreIndex\n",
    "from llama_index.core.node_parser import MarkdownNodeParser, HierarchicalNodeParser, SemanticSplitterNodeParser\n",
    "from llama_index.readers.docling import DoclingReader\n",
    "from llama_index.vector_stores.milvus import MilvusVectorStore\n",
    "from llama_index.core.chat_engine import CondenseQuestionChatEngine\n",
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "from llama_index.readers.docling import DoclingReader\n",
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n"
     ]
    }
   ],
   "source": [
    "#\"\"\n",
    "'''BAAI/bge-small-en-v1.5 - Beijing Academy of Artificial Intelligence (BAAI)\n",
    "Sentence Embedding Model / Text Embedding Model\n",
    "Specifically designed for English text.\n",
    "Based on E5 architecture, which itself is a modification of the MiniLM \n",
    "(or similar lightweight Transformer) architecture optimized for embedding tasks.\n",
    "~60 million parameters'''\n",
    "\n",
    "EMBED_MODEL = HuggingFaceEmbedding(\"BAAI/bge-small-en-v1.5\")\n",
    "embed_dim = len(EMBED_MODEL.get_text_embedding(\"Burger\"))#\n",
    "print(embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE = \"Guardrail_Scenarios.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:47:51,040 [DEBUG][_create_connection]: Created new connection using: 161cc7a226b64520aea23dc1a2a85a29 (async_milvus_client.py:599)\n"
     ]
    }
   ],
   "source": [
    "reader = DoclingReader()\n",
    "document = reader.load_data(SOURCE) \n",
    "\n",
    "node_parser_mk = MarkdownNodeParser()\n",
    "\n",
    "node_parser_semantic = SemanticSplitterNodeParser(\n",
    "    buffer_size=1, breakpoint_percentile_threshold=95, embed_model=EMBED_MODEL\n",
    ")\n",
    "\n",
    "#semantic_node_parser = SemanticSplitterNodeParser()\n",
    "MILVUS_URI = str(Path(mkdtemp())/ 'docling_ahtsham.db')\n",
    "vector_store = MilvusVectorStore(uri=MILVUS_URI,dim=embed_dim,overwrite=True)\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents=document,\n",
    "    transformations=[node_parser_mk, node_parser_semantic],\n",
    "    storage_context=StorageContext.from_defaults(vector_store=vector_store),\n",
    "    embed_model=EMBED_MODEL,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['gpt-3.5-turbo','text-davinci-003']\n",
    "\n",
    "llm = OpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0,\n",
    "    max_tokens=1024,\n",
    "    frequency_penalty=0\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: when was the last olympic held?\n",
      "A: The last Olympic Games were held in 2021.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#QUERY =  'How do you ensure software compliance (licensing) in an organization '\n",
    "QUERY = \"when was the last olympic held?\"\n",
    "query_engine = index.as_query_engine(similarity_top_k=10,llm=llm)\n",
    "result = query_engine.query(QUERY)\n",
    "print(f\"Q: {QUERY}\\nA: {result.response.strip()}\")\n",
    "#display([(n.text, n.metadata) for n in result.source_nodes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building Guardrails using Guardrails AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: guardrails-ai 0.3.1\n",
      "Uninstalling guardrails-ai-0.3.1:\n",
      "  Would remove:\n",
      "    /opt/anaconda3/envs/langchain_env/bin/guardrails\n",
      "    /opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages/guardrails/*\n",
      "    /opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages/guardrails_ai-0.3.1.dist-info/*\n",
      "Proceed (Y/n)? ^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip uninstall guardrails-ai\n",
    "pip install \"guardrails-ai>=0.4.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guardrails hub install hub://guardrails/competitor_check --no-install-local-models -q\n",
    "guardrails hub install hub://guardrails/detect_pii --no-install-local-models -q\n",
    "guardrails hub install hub://tryolabs/restricttotopic --no-install-local-models -q\n",
    "guardrails hub install hub://groundedai/grounded_ai_hallucination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages/guardrails/hub/install.py:6: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    }
   ],
   "source": [
    "import guardrails as gd\n",
    "from guardrails.hub import CompetitorCheck, DetectPII\n",
    "from guardrails.integrations.llama_index import GuardrailsQueryEngine\n",
    "from guardrails import OnFailAction\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Competitor check and PII"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SafeResponse(BaseModel):\n",
    "    response : str = Field(description=\"The safe and appropriate response to the user's query.\",\n",
    "    validators=[\n",
    "            CompetitorCheck(\n",
    "                competitors=[\"Global AI Corp\", \"Global AI\", \"GlobalAICorp\"],\n",
    "                on_fail=OnFailAction.REASK # If competitor is mentioned, reask the LLM\n",
    "            ),\n",
    "            DetectPII(\n",
    "                pii_entities=[\"EMAIL_ADDRESS\", \"PHONE_NUMBER\"], # Detect emails and phone numbers\n",
    "                on_fail=OnFailAction.FIX # Try to fix/redact PII\n",
    "            )\n",
    "        ]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "guard =  gd.Guard.for_pydantic(output_class=SafeResponse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "guard_query_engine = GuardrailsQueryEngine(query_engine,guard=guard)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Roman Colosseum is around 1,943 years old.\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "Validation failed: Validation failed: None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages/guardrails/integrations/llama_index/guardrails_query_engine.py:75\u001b[0m, in \u001b[0;36mGuardrailsQueryEngine._query\u001b[0;34m(self, query_bundle)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m validated_output\u001b[39m.\u001b[39mvalidation_passed:\n\u001b[0;32m---> 75\u001b[0m     \u001b[39mraise\u001b[39;00m ValidationError(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mValidation failed: \u001b[39m\u001b[39m{\u001b[39;00mvalidated_output\u001b[39m.\u001b[39merror\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     76\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_response_metadata(validated_output)\n",
      "\u001b[0;31mValidationError\u001b[0m: Validation failed: None",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m/Users/A200154990/CSK/CSKBOT/guardrails_check.ipynb Cell 14\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/A200154990/CSK/CSKBOT/guardrails_check.ipynb#X41sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m response_without_guard \u001b[39m=\u001b[39m query_engine\u001b[39m.\u001b[39mquery(Query)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/A200154990/CSK/CSKBOT/guardrails_check.ipynb#X41sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(response_without_guard)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/A200154990/CSK/CSKBOT/guardrails_check.ipynb#X41sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m response_with_guard \u001b[39m=\u001b[39m guard_query_engine\u001b[39m.\u001b[39;49mquery(Query)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/A200154990/CSK/CSKBOT/guardrails_check.ipynb#X41sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(response_with_guard)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages/llama_index_instrumentation/dispatcher.py:319\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    316\u001b[0m             _logger\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFailed to reset active_span_id: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    318\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 319\u001b[0m     result \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    320\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(result, asyncio\u001b[39m.\u001b[39mFuture):\n\u001b[1;32m    321\u001b[0m         \u001b[39m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[1;32m    322\u001b[0m         new_future \u001b[39m=\u001b[39m asyncio\u001b[39m.\u001b[39mensure_future(result)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages/llama_index/core/base/base_query_engine.py:52\u001b[0m, in \u001b[0;36mBaseQueryEngine.query\u001b[0;34m(self, str_or_query_bundle)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(str_or_query_bundle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m     51\u001b[0m         str_or_query_bundle \u001b[39m=\u001b[39m QueryBundle(str_or_query_bundle)\n\u001b[0;32m---> 52\u001b[0m     query_result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_query(str_or_query_bundle)\n\u001b[1;32m     53\u001b[0m dispatcher\u001b[39m.\u001b[39mevent(\n\u001b[1;32m     54\u001b[0m     QueryEndEvent(query\u001b[39m=\u001b[39mstr_or_query_bundle, response\u001b[39m=\u001b[39mquery_result)\n\u001b[1;32m     55\u001b[0m )\n\u001b[1;32m     56\u001b[0m \u001b[39mreturn\u001b[39;00m query_result\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages/llama_index_instrumentation/dispatcher.py:319\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    316\u001b[0m             _logger\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFailed to reset active_span_id: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    318\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 319\u001b[0m     result \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    320\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(result, asyncio\u001b[39m.\u001b[39mFuture):\n\u001b[1;32m    321\u001b[0m         \u001b[39m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[1;32m    322\u001b[0m         new_future \u001b[39m=\u001b[39m asyncio\u001b[39m.\u001b[39mensure_future(result)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langchain_env/lib/python3.10/site-packages/guardrails/integrations/llama_index/guardrails_query_engine.py:101\u001b[0m, in \u001b[0;36mGuardrailsQueryEngine._query\u001b[0;34m(self, query_bundle)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mUnsupported response type\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    100\u001b[0m \u001b[39mexcept\u001b[39;00m ValidationError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 101\u001b[0m     \u001b[39mraise\u001b[39;00m ValidationError(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mValidation failed: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(e)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    102\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    103\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAn error occurred during query processing: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(e)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mValidationError\u001b[0m: Validation failed: Validation failed: None"
     ]
    }
   ],
   "source": [
    "Query = \"How old is the Roman Colloseum\"\n",
    "response_without_guard = query_engine.query(Query)\n",
    "print(response_without_guard)\n",
    "\n",
    "response_with_guard = guard_query_engine.query(Query)\n",
    "print(response_with_guard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query 1: What does InnovateTech do?\n",
      "An error occurred during query: 'NoneType' object has no attribute 'validated_output'\n",
      "\n",
      "Query 2: Who is InnovateTech's main competitor?\n",
      "An error occurred during query: Validation failed: Validation failed: None\n",
      "\n",
      "Query 3: How can I contact InnovateTech support?\n",
      "An error occurred during query: Validation failed: Validation failed: None\n",
      "\n",
      "Query 4: Tell me about InnovateTech's partnerships.\n",
      "An error occurred during query: Validation failed: Validation failed: None\n",
      "\n",
      "Query 5: What is the capital of France?\n",
      "An error occurred during query: Validation failed: Validation failed: None\n"
     ]
    }
   ],
   "source": [
    "test_queries = [\n",
    "    \"What does InnovateTech do?\",\n",
    "    \"Who is InnovateTech's main competitor?\",\n",
    "    \"How can I contact InnovateTech support?\",\n",
    "    \"Tell me about InnovateTech's partnerships.\",\n",
    "    \"What is the capital of France?\"\n",
    "]\n",
    "\n",
    "for i, query_text in enumerate(test_queries):\n",
    "    print(f\"\\nQuery {i+1}: {query_text}\")\n",
    "    try:\n",
    "        response = guard_query_engine.query(query_text)\n",
    "        \n",
    "        # Access the validated output directly from the Guardrails response\n",
    "        validated_output = response.response.validated_output\n",
    "        print(f\"Validated Response: {validated_output.response}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during query: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hallucination with MNLi pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_validator(name=\"is_grounded_in_context\", data_type=\"string\")\n",
    "class IsGroundedInContext(Validator):\n",
    "    def __init__(self, nli_confidence_threshold: float = 0.7,\n",
    "                 on_fail: OnFailAction = OnFailAction.REASK, **kwargs):\n",
    "        super().__init__(name=\"is_grounded_in_context\", on_fail=on_fail, **kwargs)\n",
    "        self.nli_confidence_threshold = nli_confidence_threshold\n",
    "        # Initialize NLI model ONCE when the validator is created\n",
    "        self.nli_classifier = pipeline(\"text-classification\", model=\"MoritzLaurer/deberta-v3-small-mnli\", device=-1) # Use -1 for CPU, 0 for GPU\n",
    "\n",
    "    def validate_and_fix(self, text: str, metadata: dict, *args, **kwargs) -> ValidationResult:\n",
    "        \"\"\"\n",
    "        Validates if the text is grounded in the provided context from metadata.\n",
    "        `metadata` will contain `retrieved_context_nodes` from GuardrailsQueryEngine.\n",
    "        \"\"\"\n",
    "        retrieved_context_nodes = metadata.get(\"retrieved_context_nodes\", [])\n",
    "        if not retrieved_context_nodes:\n",
    "            # If no context, or no relevant nodes retrieved, consider it a failure for grounding.\n",
    "            return FailResult(\n",
    "                outcome=\"fail\",\n",
    "                metadata={\"reason\": \"No retrieved context available for groundedness check.\"},\n",
    "                fix_value=\"I cannot answer this question as I lack sufficient contextual information to ensure groundedness.\"\n",
    "            )\n",
    "\n",
    "        # Combine all retrieved text into a single premise for simplicity\n",
    "        # For more robustness, you might compare each sentence in 'text' against each 'chunk' individually.\n",
    "        full_context_text = \" \".join([node.get_text() for node in retrieved_context_nodes])\n",
    "\n",
    "        # Split the generated text into claims/sentences for individual checking\n",
    "        # Basic split by period for demo. Use a proper sentence tokenizer for production.\n",
    "        generated_claims = [s.strip() for s in text.split('.') if s.strip()]\n",
    "\n",
    "        ungrounded_claims = []\n",
    "        for claim in generated_claims:\n",
    "            # Check if this claim is entailed by the full context using NLI model\n",
    "            nli_result = self.nli_classifier(f\"{full_context_text} {claim}\")\n",
    "            \n",
    "            # The result is typically a list of dicts, e.g., [{'label': 'ENTAILMENT', 'score': 0.95}]\n",
    "            if nli_result and nli_result[0]['label'] == 'ENTAILMENT' and nli_result[0]['score'] >= self.nli_confidence_threshold:\n",
    "                # This claim is grounded\n",
    "                continue\n",
    "            else:\n",
    "                ungrounded_claims.append(claim)\n",
    "\n",
    "        if ungrounded_claims:\n",
    "            return FailResult(\n",
    "                outcome=\"fail\",\n",
    "                metadata={\"ungrounded_claims\": ungrounded_claims, \"reason\": \"Generated text contains claims not grounded in context.\"},\n",
    "                fix_value=None # Let Guardrails reask the LLM\n",
    "            )\n",
    "        else:\n",
    "            return PassResult(outcome=\"pass\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "entailment_model = 'GuardrailsAI/finetuned_nli_provenance'\n",
    "NLI_PIPELINE = pipeline(\"text-classification\", model=entailment_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keeping it on the topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from guardrails.hub import GroundedAIHallucination,RestrictToTopic,CompetitorCheck,DetectPII\n",
    "# fro zero shot classisfication\n",
    "from transformers import pipeline\n",
    "from guardrails.validator_base import FailResult, PassResult, ValidationResult, Validator, register_validator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building Custom nli pipeline for topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "# Guardrails AI imports\n",
    "import guardrails as gd\n",
    "from guardrails.integrations.llama_index import GuardrailsQueryEngine\n",
    "from guardrails.hub import CompetitorCheck, DetectPII\n",
    "from guardrails import OnFailAction\n",
    "from guardrails.validator_base import FailResult, PassResult, ValidationResult, Validator, register_validator\n",
    "\n",
    "# Pydantic imports\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# For NLI model (Hugging Face Transformers) - for hallucination and zero-shot topic\n",
    "from transformers import pipeline\n",
    "\n",
    "# --- Custom Hallucination Validator (from previous response, re-confirmed) ---\n",
    "@register_validator(name=\"is_on_topic_bart\", data_type=\"string\")\n",
    "class IsOnTopicBart(Validator):\n",
    "    def __init__(self, allowed_topics: List[str], topic_threshold: float = 0.7, on_fail: OnFailAction = OnFailAction.REASK, **kwargs):\n",
    "        super().__init__(name=\"is_on_topic_bart\", on_fail=on_fail, **kwargs)\n",
    "        self.allowed_topics = allowed_topics\n",
    "        self.topic_threshold = topic_threshold\n",
    "        self.classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\", device=-1)\n",
    "\n",
    "    def validate_and_fix(self, text: str, *args, **kwargs) -> ValidationResult:\n",
    "        if not text.strip():\n",
    "            return PassResult(outcome=\"pass\", metadata={\"reason\": \"Empty response, skipping topic check.\"})\n",
    "\n",
    "        result = self.classifier(text, self.allowed_topics, multi_label=False)\n",
    "        predicted_label = result['labels'][0]\n",
    "        predicted_score = result['scores'][0]\n",
    "\n",
    "        if predicted_score >= self.topic_threshold and predicted_label in self.allowed_topics:\n",
    "            return PassResult(outcome=\"pass\", metadata={\"classified_topic\": predicted_label, \"score\": predicted_score})\n",
    "        else:\n",
    "            return FailResult(\n",
    "                outcome=\"fail\",\n",
    "                metadata={\"classified_topic\": predicted_label, \"score\": predicted_score,\n",
    "                          \"reason\": f\"Response topic '{predicted_label}' (score: {predicted_score:.2f}) is below threshold or not strong enough for allowed topics.\"},\n",
    "                fix_value=None\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Guardraila Hub\n",
    "Using inbuilt Guardrails AI functions from guardrail.hub. For more from hub you can look into https://hub.guardrailsai.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM and Embedding model configured for LlamaIndex.\n",
      "Dummy data created in 'data/' directory.\n",
      "LlamaIndex VectorStoreIndex created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "# Guardrails AI imports\n",
    "import guardrails as gd\n",
    "from guardrails.integrations.llama_index import GuardrailsQueryEngine\n",
    "from guardrails import OnFailAction\n",
    "\n",
    "# --- Import Validators directly from Guardrails Hub ---\n",
    "from guardrails.hub import CompetitorCheck\n",
    "# For topic adherence\n",
    "from guardrails.hub import RestrictToTopic\n",
    "# For hallucination check\n",
    "from guardrails.hub import GroundedAIHallucination\n",
    "\n",
    "# Pydantic imports\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# --- No custom validators needed for topic or hallucination anymore ---\n",
    "# We are using the Hub validators which encapsulate the logic.\n",
    "\n",
    "\n",
    "# 1. Load Environment Variables\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\"OPENAI_API_KEY not found. Please set it in your .env file.\")\n",
    "\n",
    "# 2. Configure LlamaIndex LLM and Embedding Model\n",
    "Settings.llm = OpenAI(model=\"gpt-3.5-turbo\", api_key=OPENAI_API_KEY, temperature=0.0)\n",
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\",\n",
    "    cache_folder=\"./models\"\n",
    ")\n",
    "print(\"LLM and Embedding model configured for LlamaIndex.\")\n",
    "\n",
    "# 3. Create a dummy data directory and file\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "with open(\"data/company_info.txt\", \"w\") as f:\n",
    "    f.write(\"\"\"\n",
    "Our company, InnovateTech, specializes in cutting-edge AI solutions.\n",
    "We are a leader in natural language processing and computer vision.\n",
    "InnovateTech was founded in 2015 by Dr. Elena Petrova.\n",
    "Our main competitor is Global AI Corp. We aim to offer better value than Global AI Corp.\n",
    "For support, contact support@innovatetech.com or call 555-123-4567.\n",
    "InnovateTech also has a partnership with ResearchLabs Inc.\n",
    "\"\"\")\n",
    "print(\"Dummy data created in 'data/' directory.\")\n",
    "\n",
    "# 4. Load Documents and Create LlamaIndex VectorStoreIndex\n",
    "documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "print(\"LlamaIndex VectorStoreIndex created.\")\n",
    "\n",
    "# 5. Define Pydantic Model with Hub Validators\n",
    "class SafeResponse(BaseModel):\n",
    "    response: str = Field(\n",
    "        description=\"The safe and appropriate response to the user's query.\",\n",
    "        validators=[\n",
    "            CompetitorCheck(\n",
    "                competitors=[\"Global AI Corp\", \"Global AI\", \"GlobalAICorp\"],\n",
    "                on_fail=OnFailAction.REASK\n",
    "            ),\n",
    "            # --- RestrictToTopic for Topic Adherence ---\n",
    "            # Using 'facebook/bart-large-mnli' as the model for zero-shot classification\n",
    "            # disable_llm=True ensures it only uses the classifier, not an LLM fallback.\n",
    "            RestrictToTopic(\n",
    "                valid_topics=[\"Technology\", \"Business\", \"Support\", \"Partnerships\", \"Company History\"],\n",
    "                invalid_topics=[\"music\"],\n",
    "                model=\"facebook/bart-large-mnli\", # Specify BART model for classification\n",
    "                disable_llm=True,                 # Disable LLM fallback, rely on BART classifier\n",
    "                model_threshold=0.8,              # Confidence threshold for topic adherence\n",
    "                on_fail=OnFailAction.REASK\n",
    "            ),\n",
    "            # --- GroundedAIHallucination for Hallucination Check ---\n",
    "            # This validator uses a fine-tuned model (often NLI-based) internally.\n",
    "            # It expects 'query' and 'reference' (context) in metadata.\n",
    "            # LlamaIndex's GuardrailsQueryEngine passes 'retrieved_context_nodes' in metadata.\n",
    "            # GroundedAIHallucination expects 'reference' to be a string or list of strings.\n",
    "            # We will ensure the metadata is correctly formatted.\n",
    "            GroundedAIHallucination(quant=False,\n",
    "                on_fail=OnFailAction.REASK\n",
    "                # This validator often uses its own internal NLI-like model\n",
    "                # based on its implementation from GroundedAI.\n",
    "                # No direct 'mnli_model' parameter here as it's encapsulated.\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "# 6. Create the Guardrails AI Guard object\n",
    "guard = gd.Guard.for_pydantic(output_class=SafeResponse)\n",
    "print(\"Guardrails AI Guard created with Competitor, RestrictToTopic, and GroundedAIHallucination validators.\")\n",
    "\n",
    "# 7. Create the LlamaIndex QueryEngine and wrap it with GuardrailsQueryEngine\n",
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k=5\n",
    ")\n",
    "\n",
    "# This is a crucial step for GroundedAIHallucination.\n",
    "# GuardrailsQueryEngine automatically passes `retrieved_context_nodes` from LlamaIndex.\n",
    "# However, GroundedAIHallucination typically expects a 'reference' key in metadata,\n",
    "# which needs to be the text content. We will transform the nodes into a string for it.\n",
    "class CustomGuardrailsQueryEngine(GuardrailsQueryEngine):\n",
    "    async def _acall(self, query: str) -> Any:\n",
    "        # First, call the base query engine to get the LLM response and source nodes\n",
    "        llama_response = await self.query_engine.aquery(query)\n",
    "        \n",
    "        # Prepare metadata for Guardrails validators\n",
    "        # GroundedAIHallucination expects 'query' and 'reference'.\n",
    "        # 'reference' should be a string of the context.\n",
    "        # 'retrieved_context_nodes' is what GuardrailsQueryEngine typically passes.\n",
    "        retrieved_context_texts = [node.get_text() for node in llama_response.source_nodes]\n",
    "        \n",
    "        # Add the 'query' and 'reference' (context) to the metadata\n",
    "        # This aligns with what GroundedAIHallucination expects\n",
    "        # The GuardrailsQueryEngine will merge this with its default metadata.\n",
    "        # We also pass the original retrieved_context_nodes, as some validators might use them raw.\n",
    "        metadata_for_guard = {\n",
    "            \"query\": query,\n",
    "            \"reference\": \" \".join(retrieved_context_texts), # GroundedAIHallucination expects a string\n",
    "            \"retrieved_context_nodes\": llama_response.source_nodes # Keep for other validators if needed\n",
    "        }\n",
    "\n",
    "        # Call the Guardrails Guard with the LLM's response and prepared metadata\n",
    "        validated_output = await self.guard.validate(\n",
    "            llm_output=llama_response.response, # The LLM's raw response\n",
    "            metadata=metadata_for_guard\n",
    "        )\n",
    "\n",
    "        # The GuardrailsQueryEngine expects a Guardrails LlamaIndex Response object\n",
    "        # It handles wrapping the validated_output in the correct LlamaIndex Response type.\n",
    "        return self._create_response_from_guard_result(\n",
    "            validated_output=validated_output,\n",
    "            raw_response=llama_response.response,\n",
    "            metadata=metadata_for_guard # Include metadata if you want it in the final response\n",
    "        )\n",
    "\n",
    "guardrails_query_engine = CustomGuardrailsQueryEngine(\n",
    "    query_engine=query_engine,\n",
    "    guard=guard\n",
    ")\n",
    "print(\"LlamaIndex QueryEngine wrapped with GuardrailsQueryEngine.\")\n",
    "print(\"Custom GuardrailsQueryEngine created to pass 'query' and 'reference' metadata for GroundedAIHallucination.\")\n",
    "\n",
    "# 8. Test Queries with Guardrails\n",
    "print(\"\\n--- Testing Queries with Guardrails ---\")\n",
    "\n",
    "test_queries = [\n",
    "    \"What does InnovateTech do?\", # On-topic & Grounded\n",
    "    \"Who is InnovateTech's main competitor?\", # On-topic & Grounded\n",
    "    \"How can I contact InnovateTech support?\", # On-topic & Grounded\n",
    "    \"Tell me about InnovateTech's partnerships.\", # On-topic & Grounded\n",
    "    \"When was InnovateTech founded?\", # On-topic & Grounded\n",
    "    \"What is the capital of France?\", # Off-topic & Ungrounded\n",
    "    \"InnovateTech invented time travel in 2020.\", # On-topic (Company) but Ungrounded\n",
    "    \"Tell me about the weather today in Mumbai.\" # Clearly off-topic\n",
    "]\n",
    "\n",
    "# Use an async loop to run queries\n",
    "import asyncio\n",
    "\n",
    "async def run_queries():\n",
    "    for i, query_text in enumerate(test_queries):\n",
    "        print(f\"\\nQuery {i+1}: {query_text}\")\n",
    "        try:\n",
    "            # Await the async query\n",
    "            response = await guardrails_query_engine.aquery(query_text)\n",
    "            \n",
    "            validated_output = response.response.validated_output\n",
    "            print(f\"Validated Response: {validated_output.response}\")\n",
    "\n",
    "            if guard.history.last:\n",
    "                print(f\"  Guardrails Outcome: {guard.history.last.outcome}\")\n",
    "                if guard.history.last.validation_result and guard.history.last.validation_result.errors:\n",
    "                    print(f\"  Guardrails Errors: {guard.history.last.validation_result.errors}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during query: {e}\")\n",
    "\n",
    "# Run the async function\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(run_queries())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
